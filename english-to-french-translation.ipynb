{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:04:27.755855: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-13 16:04:27.771866: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-13 16:04:27.776704: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-13 16:04:27.788286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-13 16:04:28.711540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 200\n",
    "latent_dim = 256\n",
    "num_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, target_text,_= line.split('\\t')\n",
    "    target_text = '\\t'+target_text+'\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.',\n",
       " 'Hi.',\n",
       " 'Hi.',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Who?',\n",
       " 'Wow!',\n",
       " 'Fire!',\n",
       " 'Help!',\n",
       " 'Jump.',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Wait!',\n",
       " 'Wait!',\n",
       " 'Go on.',\n",
       " 'Go on.',\n",
       " 'Go on.',\n",
       " 'Hello!',\n",
       " 'Hello!',\n",
       " 'I see.',\n",
       " 'I try.',\n",
       " 'I won!',\n",
       " 'I won!',\n",
       " 'I won.',\n",
       " 'Oh no!',\n",
       " 'Attack!',\n",
       " 'Attack!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Get up.',\n",
       " 'Go now.',\n",
       " 'Go now.',\n",
       " 'Go now.',\n",
       " 'Got it!',\n",
       " 'Got it!',\n",
       " 'Got it?',\n",
       " 'Got it?',\n",
       " 'Got it?',\n",
       " 'Hop in.',\n",
       " 'Hop in.',\n",
       " 'Hug me.',\n",
       " 'Hug me.',\n",
       " 'I fell.',\n",
       " 'I fell.',\n",
       " 'I know.',\n",
       " 'I left.',\n",
       " 'I left.',\n",
       " 'I lied.',\n",
       " 'I lost.',\n",
       " 'I paid.',\n",
       " \"I'm 19.\",\n",
       " \"I'm OK.\",\n",
       " \"I'm OK.\",\n",
       " 'Listen.',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Thanks.',\n",
       " 'We try.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'Ask Tom.',\n",
       " 'Awesome!',\n",
       " 'Be calm.',\n",
       " 'Be calm.',\n",
       " 'Be calm.',\n",
       " 'Be cool.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be kind.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Beat it.',\n",
       " 'Call me.',\n",
       " 'Call me.',\n",
       " 'Call us.',\n",
       " 'Call us.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come on!',\n",
       " 'Come on.',\n",
       " 'Come on.',\n",
       " 'Come on.',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Get Tom.',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go slow.',\n",
       " 'Go slow.',\n",
       " 'Goodbye!',\n",
       " 'Goodbye!',\n",
       " 'Hang on!',\n",
       " 'Hang on!',\n",
       " 'Hang on.',\n",
       " 'Hang on.',\n",
       " 'He quit.',\n",
       " 'He quit.',\n",
       " 'He runs.',\n",
       " 'Help me!',\n",
       " 'Help me.',\n",
       " 'Help me.',\n",
       " 'Help us.',\n",
       " 'Help us.',\n",
       " 'Hold it!',\n",
       " 'Hold on.',\n",
       " 'Hug Tom.',\n",
       " 'I agree.',\n",
       " 'I cried.',\n",
       " 'I dozed.',\n",
       " 'I dozed.',\n",
       " 'I drive.',\n",
       " 'I smoke.',\n",
       " 'I snore.',\n",
       " 'I stink.',\n",
       " 'I stood.',\n",
       " 'I stood.',\n",
       " 'I swore.',\n",
       " 'I swore.',\n",
       " 'I tried.',\n",
       " 'I tried.',\n",
       " 'I tried.',\n",
       " 'I waved.',\n",
       " \"I'll go.\",\n",
       " \"I'm Tom.\",\n",
       " \"I'm fat.\",\n",
       " \"I'm fat.\",\n",
       " \"I'm fit.\",\n",
       " \"I'm hit!\",\n",
       " \"I'm hit!\",\n",
       " \"I'm ill.\",\n",
       " \"I'm sad.\",\n",
       " \"I'm shy.\",\n",
       " \"I'm wet.\",\n",
       " \"I'm wet.\",\n",
       " \"It's me!\",\n",
       " 'Join us.',\n",
       " 'Join us.',\n",
       " 'Keep it.',\n",
       " 'Keep it.',\n",
       " 'Kiss me.',\n",
       " 'Kiss me.',\n",
       " 'Me, too.',\n",
       " 'Open up.',\n",
       " 'Open up.',\n",
       " 'Perfect!',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you.',\n",
       " 'Show me.',\n",
       " 'Show me.',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Skip it.',\n",
       " 'So long.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Tell me.',\n",
       " 'Tell me.',\n",
       " 'Tom won.',\n",
       " 'Wake up!',\n",
       " 'Wake up!',\n",
       " 'Wake up!',\n",
       " 'Wake up.',\n",
       " 'Wake up.',\n",
       " 'Wash up.',\n",
       " 'Wash up.',\n",
       " 'We know.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'Welcome.',\n",
       " 'Who won?',\n",
       " 'Who won?',\n",
       " 'You run.',\n",
       " 'Am I fat?',\n",
       " 'Am I fat?',\n",
       " 'Ask them.',\n",
       " 'Ask them.',\n",
       " 'Back off!',\n",
       " 'Back off!',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Be a man.',\n",
       " 'Be a man.',\n",
       " 'Be still.',\n",
       " 'Be still.',\n",
       " 'Be still.',\n",
       " 'Beats me.',\n",
       " 'Beats me.',\n",
       " 'Call Tom.',\n",
       " 'Call Tom.',\n",
       " 'Cheer up!',\n",
       " 'Cool off!',\n",
       " 'Cuff him.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Find Tom.',\n",
       " 'Find Tom.',\n",
       " 'Fix this.',\n",
       " 'Fix this.',\n",
       " 'Get down!',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get real!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Grab him.',\n",
       " 'Grab him.',\n",
       " 'Have fun.',\n",
       " 'Have fun.',\n",
       " 'He tries.',\n",
       " \"He's wet.\",\n",
       " 'Help Tom.',\n",
       " 'Help Tom.',\n",
       " 'Hi, guys.',\n",
       " 'How cute!',\n",
       " 'How deep?',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'I did OK.',\n",
       " 'I did OK.',\n",
       " 'I did it.',\n",
       " 'I did it.',\n",
       " 'I failed.',\n",
       " 'I forgot.',\n",
       " 'I get it.',\n",
       " 'I got it.',\n",
       " 'I got it.',\n",
       " 'I helped.',\n",
       " 'I jumped.',\n",
       " 'I looked.',\n",
       " 'I moaned.',\n",
       " 'I nodded.',\n",
       " 'I obeyed.',\n",
       " 'I phoned.',\n",
       " 'I phoned.',\n",
       " 'I refuse.',\n",
       " 'I refuse.',\n",
       " 'I rested.',\n",
       " 'I rested.',\n",
       " 'I saw it.',\n",
       " 'I saw it.',\n",
       " 'I sighed.',\n",
       " 'I stayed.',\n",
       " 'I stayed.',\n",
       " 'I talked.',\n",
       " 'I use it.',\n",
       " 'I use it.',\n",
       " 'I use it.',\n",
       " \"I'll pay.\",\n",
       " \"I'll pay.\",\n",
       " \"I'll try.\",\n",
       " \"I'll try.\",\n",
       " \"I'm back.\",\n",
       " \"I'm back.\",\n",
       " \"I'm bald.\",\n",
       " \"I'm busy.\",\n",
       " \"I'm busy.\",\n",
       " \"I'm calm.\",\n",
       " \"I'm cold.\",\n",
       " \"I'm cool.\",\n",
       " \"I'm cool.\",\n",
       " \"I'm deaf.\",\n",
       " \"I'm deaf.\",\n",
       " \"I'm done.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fast.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm free!\",\n",
       " \"I'm free.\",\n",
       " \"I'm free.\",\n",
       " \"I'm full.\",\n",
       " \"I'm full.\",\n",
       " \"I'm game.\",\n",
       " \"I'm game.\",\n",
       " \"I'm glad.\",\n",
       " \"I'm home.\",\n",
       " \"I'm late.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm okay.\",\n",
       " \"I'm okay.\",\n",
       " \"I'm rich.\",\n",
       " \"I'm safe.\",\n",
       " \"I'm sick.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm tall.\",\n",
       " \"I'm thin.\",\n",
       " \"I'm tidy.\",\n",
       " \"I'm tidy.\",\n",
       " \"I'm ugly.\",\n",
       " \"I'm ugly.\",\n",
       " \"I'm weak.\",\n",
       " \"I'm well.\",\n",
       " \"I'm well.\",\n",
       " \"I've won.\",\n",
       " \"I've won.\",\n",
       " 'It helps.',\n",
       " 'It hurts.',\n",
       " 'It works.',\n",
       " 'It works.',\n",
       " \"It's Tom.\",\n",
       " \"It's fun.\",\n",
       " \"It's fun.\",\n",
       " \"It's his.\",\n",
       " \"It's his.\",\n",
       " \"It's new.\",\n",
       " \"It's new.\",\n",
       " \"It's odd.\",\n",
       " \"It's red.\",\n",
       " \"It's sad.\",\n",
       " 'Keep out!',\n",
       " 'Keep out.',\n",
       " 'Kiss Tom.',\n",
       " 'Leave it.',\n",
       " 'Leave it.',\n",
       " 'Leave me.',\n",
       " 'Leave us.',\n",
       " 'Leave us.',\n",
       " \"Let's go!\",\n",
       " \"Let's go.\",\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Marry me.',\n",
       " 'Marry me.',\n",
       " 'May I go?',\n",
       " 'May I go?',\n",
       " 'May I go?',\n",
       " 'Save Tom.',\n",
       " 'Save Tom.',\n",
       " 'Say what?',\n",
       " 'She came.',\n",
       " 'She died.',\n",
       " 'She runs.',\n",
       " 'Sit down!',\n",
       " 'Sit down!',\n",
       " 'Sit down.',\n",
       " 'Sit here.',\n",
       " 'Sit here.',\n",
       " 'Speak up!',\n",
       " 'Speak up!',\n",
       " 'Stand up.',\n",
       " 'Stop Tom.',\n",
       " 'Stop Tom.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Tell Tom.',\n",
       " 'Tell Tom.',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'Tom came.',\n",
       " 'Tom died.',\n",
       " 'Tom knew.',\n",
       " 'Tom left.',\n",
       " 'Tom left.',\n",
       " 'Tom lied.',\n",
       " 'Tom lies.',\n",
       " 'Tom lost.',\n",
       " 'Tom paid.',\n",
       " 'Too late.',\n",
       " 'Trust me.',\n",
       " 'Trust me.',\n",
       " 'Try hard.',\n",
       " 'Try some.',\n",
       " 'Try some.',\n",
       " 'Try this.',\n",
       " 'Try this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Warn Tom.',\n",
       " 'Warn Tom.',\n",
       " 'Watch me.',\n",
       " 'Watch me.',\n",
       " 'Watch us.',\n",
       " 'Watch us.',\n",
       " 'We agree.',\n",
       " \"We'll go.\",\n",
       " \"We're OK.\",\n",
       " 'What for?',\n",
       " 'What for?',\n",
       " 'What fun!',\n",
       " 'What fun!',\n",
       " 'Who came?',\n",
       " 'Who died?',\n",
       " 'Who fell?',\n",
       " 'Who lost?',\n",
       " 'Who quit?',\n",
       " \"Who's he?\",\n",
       " 'Write me.',\n",
       " 'Write me.',\n",
       " 'You lost.',\n",
       " 'You lost.',\n",
       " 'After you.',\n",
       " 'Aim. Fire!',\n",
       " 'Am I late?',\n",
       " 'Answer me.',\n",
       " 'Be seated.',\n",
       " 'Be seated.',\n",
       " 'Birds fly.',\n",
       " 'Bless you.',\n",
       " 'Call home!',\n",
       " 'Calm down!',\n",
       " 'Calm down.',\n",
       " 'Can we go?',\n",
       " 'Can we go?',\n",
       " 'Can we go?',\n",
       " 'Catch Tom.',\n",
       " 'Catch Tom.',\n",
       " 'Catch him.',\n",
       " 'Chill out.',\n",
       " 'Come back.',\n",
       " 'Come back.',\n",
       " 'Come here.',\n",
       " 'Come here.',\n",
       " 'Come over!',\n",
       " 'Come over!',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come soon.',\n",
       " 'Come soon.',\n",
       " 'Cool down.',\n",
       " 'Did I win?',\n",
       " 'Did I win?',\n",
       " 'Did I win?',\n",
       " 'Do it now.',\n",
       " 'Dogs bark.',\n",
       " 'Dogs bark.',\n",
       " \"Don't ask.\",\n",
       " \"Don't cry.\",\n",
       " \"Don't die.\",\n",
       " \"Don't die.\",\n",
       " \"Don't lie.\",\n",
       " \"Don't run.\",\n",
       " \"Don't run.\",\n",
       " 'Excuse me.',\n",
       " 'Excuse me.',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Fantastic!',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Follow me.',\n",
       " 'Follow us.',\n",
       " 'Follow us.',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get ready.',\n",
       " 'Get ready.',\n",
       " 'Go get it.',\n",
       " 'Go get it.',\n",
       " 'Go inside.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Good luck.',\n",
       " 'Good luck.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab this.',\n",
       " 'Grab this.',\n",
       " 'Hands off.',\n",
       " 'He is ill.',\n",
       " 'He is old.',\n",
       " \"He's a DJ.\",\n",
       " \"He's good.\",\n",
       " \"He's lazy.\",\n",
       " \"He's mine.\",\n",
       " \"He's rich.\",\n",
       " \"He's sexy.\",\n",
       " 'Here I am.',\n",
       " \"Here's $5.\",\n",
       " 'Hold fire.',\n",
       " 'Hold fire.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'How awful!',\n",
       " 'How weird!',\n",
       " \"How's Tom?\",\n",
       " \"How's Tom?\",\n",
       " 'I am cold.',\n",
       " 'I am good.',\n",
       " 'I am okay.',\n",
       " 'I am sick.',\n",
       " 'I am sure.',\n",
       " 'I am sure.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I can run.',\n",
       " 'I can ski.',\n",
       " 'I cringed.',\n",
       " 'I cringed.',\n",
       " 'I cringed.',\n",
       " 'I exhaled.',\n",
       " 'I gave up.',\n",
       " 'I give in.',\n",
       " 'I give up.',\n",
       " 'I got hot.',\n",
       " 'I got hot.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I hate it.',\n",
       " 'I have it.',\n",
       " 'I hit Tom.',\n",
       " 'I hope so.',\n",
       " 'I hurried.',\n",
       " 'I hurried.',\n",
       " 'I inhaled.',\n",
       " 'I knew it.',\n",
       " 'I like it.',\n",
       " 'I lost it.',\n",
       " 'I love it!',\n",
       " 'I love it.',\n",
       " 'I mean it!',\n",
       " 'I mean it.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I need it.',\n",
       " 'I need it.',\n",
       " 'I noticed.',\n",
       " 'I prepaid.',\n",
       " 'I promise.',\n",
       " 'I relaxed.',\n",
       " 'I relaxed.',\n",
       " 'I retired.',\n",
       " 'I said no.',\n",
       " 'I said so.',\n",
       " 'I saw him.',\n",
       " 'I saw him.',\n",
       " 'I saw him.',\n",
       " 'I saw one.',\n",
       " 'I saw one.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I see Tom.',\n",
       " 'I shouted.',\n",
       " 'I tripped.',\n",
       " 'I tripped.',\n",
       " 'I want it.',\n",
       " 'I was new.',\n",
       " 'I was new.',\n",
       " 'I will go.',\n",
       " 'I woke up.',\n",
       " 'I woke up.',\n",
       " \"I'd agree.\",\n",
       " \"I'd leave.\",\n",
       " \"I'll call.\",\n",
       " \"I'll cook.\",\n",
       " \"I'll help.\",\n",
       " \"I'll live.\",\n",
       " \"I'll obey.\",\n",
       " \"I'll pack.\",\n",
       " \"I'll pack.\",\n",
       " \"I'll pack.\",\n",
       " \"I'll pass.\",\n",
       " \"I'll quit.\",\n",
       " \"I'll sing.\",\n",
       " \"I'll stay.\",\n",
       " \"I'll stop.\",\n",
       " \"I'll swim.\",\n",
       " \"I'll talk.\",\n",
       " \"I'll talk.\",\n",
       " \"I'll wait.\",\n",
       " \"I'll walk.\",\n",
       " \"I'll work.\",\n",
       " \"I'll work.\",\n",
       " \"I'm a cop.\",\n",
       " \"I'm a man.\",\n",
       " \"I'm alive.\",\n",
       " \"I'm alive.\",\n",
       " \"I'm alive.\",\n",
       " \"I'm alone.\",\n",
       " \"I'm alone.\",\n",
       " \"I'm angry.\",\n",
       " \"I'm angry.\",\n",
       " \"I'm armed.\",\n",
       " \"I'm armed.\",\n",
       " \"I'm awake.\",\n",
       " \"I'm blind.\",\n",
       " \"I'm broke.\",\n",
       " \"I'm clean.\",\n",
       " \"I'm clean.\",\n",
       " \"I'm crazy.\",\n",
       " \"I'm crazy.\",\n",
       " \"I'm cured.\",\n",
       " \"I'm cured.\",\n",
       " \"I'm dizzy.\",\n",
       " \"I'm drunk.\",\n",
       " \"I'm drunk.\",\n",
       " \"I'm drunk.\",\n",
       " \"I'm dying.\",\n",
       " \"I'm early.\",\n",
       " \"I'm first.\",\n",
       " \"I'm fussy.\",\n",
       " \"I'm fussy.\",\n",
       " \"I'm fussy.\",\n",
       " \"I'm going.\",\n",
       " \"I'm going.\",\n",
       " \"I'm going.\",\n",
       " \"I'm going.\",\n",
       " \"I'm loyal.\",\n",
       " \"I'm loyal.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lying.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm quiet.\",\n",
       " \"I'm ready!\",\n",
       " \"I'm ready!\",\n",
       " \"I'm ready.\",\n",
       " \"I'm right.\",\n",
       " \"I'm sober.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm stuck.\",\n",
       " \"I'm timid.\",\n",
       " \"I'm tired.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm yours.\",\n",
       " \"I'm yours.\",\n",
       " \"I've lost.\",\n",
       " 'Is Tom OK?',\n",
       " 'Is Tom OK?',\n",
       " 'Is it bad?',\n",
       " 'Is it far?',\n",
       " 'Is it far?',\n",
       " 'Is it hot?',\n",
       " 'Is it you?',\n",
       " 'Is it you?',\n",
       " 'Is it you?',\n",
       " 'It failed.',\n",
       " 'It snowed.',\n",
       " 'It stinks.',\n",
       " 'It stinks.',\n",
       " 'It was OK.',\n",
       " 'It was OK.',\n",
       " 'It was OK.',\n",
       " 'It worked.',\n",
       " 'It worked.',\n",
       " \"It's 3:30.\",\n",
       " \"It's 8:30.\",\n",
       " \"It's 8:30.\",\n",
       " \"It's a TV.\",\n",
       " \"It's cold.\",\n",
       " \"It's cold.\",\n",
       " \"It's dark.\",\n",
       " \"It's dead.\",\n",
       " \"It's dead.\",\n",
       " \"It's dead.\",\n",
       " \"It's done.\",\n",
       " \"It's easy.\",\n",
       " \"It's food.\",\n",
       " \"It's free.\",\n",
       " \"It's here.\",\n",
       " \"It's here.\",\n",
       " \"It's hers.\",\n",
       " \"It's hers.\",\n",
       " \"It's late.\",\n",
       " \"It's lost.\",\n",
       " \"It's mine.\",\n",
       " \"It's mine.\",\n",
       " \"It's mine.\",\n",
       " \"It's mine.\",\n",
       " \"It's open.\",\n",
       " \"It's ours.\",\n",
       " \"It's ours.\",\n",
       " \"It's ours.\",\n",
       " \"It's sand.\",\n",
       " \"It's time.\",\n",
       " \"It's time.\",\n",
       " \"It's true!\",\n",
       " \"It's work.\",\n",
       " 'Keep calm.',\n",
       " 'Keep that.',\n",
       " 'Keep that.',\n",
       " 'Keep this.',\n",
       " 'Keep this.',\n",
       " 'Let it be.',\n",
       " 'Let it be.',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go.',\n",
       " 'Let me go.',\n",
       " 'Let me go.',\n",
       " 'Let me go.',\n",
       " 'Let me in.',\n",
       " 'Let me in.',\n",
       " \"Let's ask.\",\n",
       " \"Let's eat.\",\n",
       " \"Let's see.\",\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Look away.',\n",
       " 'Look away.',\n",
       " 'Look back!',\n",
       " 'Look back!',\n",
       " 'Look here.',\n",
       " 'Look here.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Move over.',\n",
       " 'Move over.',\n",
       " 'Move over.',\n",
       " 'Nice shot!',\n",
       " 'Of course!',\n",
       " 'Of course!',\n",
       " 'Of course!',\n",
       " 'Of course.',\n",
       " 'Of course.',\n",
       " 'Oh please!',\n",
       " 'Oh please!',\n",
       " 'Pardon me?',\n",
       " 'Pardon me?',\n",
       " 'Pardon me?',\n",
       " 'Read this.',\n",
       " 'Say hello.',\n",
       " 'See above.',\n",
       " 'See below.',\n",
       " 'See below.',\n",
       " 'Seize him!',\n",
       " 'Seize him!',\n",
       " 'Seriously?',\n",
       " 'Seriously?',\n",
       " 'Seriously?',\n",
       " 'She cried.',\n",
       " 'She cried.',\n",
       " 'She tried.',\n",
       " 'She walks.',\n",
       " \"She's hot.\",\n",
       " \"She's hot.\",\n",
       " 'Sign here.',\n",
       " 'Sign here.',\n",
       " 'Sign this.',\n",
       " 'Sign this.',\n",
       " 'Slow down.',\n",
       " 'Slow down.',\n",
       " 'Stay away.',\n",
       " 'Stay away.',\n",
       " 'Stay back.',\n",
       " 'Stay back.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay down!',\n",
       " 'Stay down!',\n",
       " 'Stay down.',\n",
       " 'Stay down.',\n",
       " 'Stay here.',\n",
       " 'Stay here.',\n",
       " 'Stay here.',\n",
       " 'Stay thin.',\n",
       " 'Step back.',\n",
       " 'Step back.',\n",
       " 'Stop that!',\n",
       " 'Stop that.',\n",
       " 'Stop that.',\n",
       " 'Stop that.',\n",
       " 'Stop them.',\n",
       " 'Take care!',\n",
       " 'Take care!',\n",
       " 'Take care.',\n",
       " 'Take care.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take this.',\n",
       " 'Take this.',\n",
       " 'Thank you.',\n",
       " 'Thank you.',\n",
       " \"That's OK.\",\n",
       " \"That's OK.\",\n",
       " \"That's OK.\",\n",
       " \"That's OK.\",\n",
       " \"That's it.\",\n",
       " 'Then what?',\n",
       " 'They fell.',\n",
       " 'They fell.',\n",
       " 'They left.',\n",
       " 'They left.',\n",
       " 'They lied.',\n",
       " 'They lied.',\n",
       " 'They lost.',\n",
       " 'They lost.',\n",
       " 'They swam.',\n",
       " 'They swam.',\n",
       " 'They swam.',\n",
       " 'They swam.',\n",
       " \"Time's up.\",\n",
       " 'Tom cooks.',\n",
       " 'Tom cried.',\n",
       " 'Tom is OK.',\n",
       " 'Tom is in.',\n",
       " 'Tom knits.',\n",
       " 'Tom knows.',\n",
       " 'Tom rocks.',\n",
       " 'Tom spoke.',\n",
       " 'Tom waved.',\n",
       " 'Tom works.',\n",
       " \"Tom's fat.\",\n",
       " \"Tom's mad.\",\n",
       " \"Tom's mad.\",\n",
       " \"Tom's sad.\",\n",
       " 'Trust Tom.',\n",
       " 'Trust Tom.',\n",
       " 'Try again.',\n",
       " 'Try again.',\n",
       " 'Try again.',\n",
       " 'Try it on.',\n",
       " 'Turn left.',\n",
       " 'Wait here.',\n",
       " 'Wait here.',\n",
       " 'Wait here.',\n",
       " 'Wait here.',\n",
       " 'Watch out!',\n",
       " 'Watch out!',\n",
       " 'Watch out!',\n",
       " 'We agreed.',\n",
       " 'We did it!',\n",
       " 'We did it.',\n",
       " 'We did it.',\n",
       " 'We forgot.',\n",
       " 'We saw it.',\n",
       " 'We saw it.',\n",
       " 'We smiled.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We waited.',\n",
       " 'We waited.',\n",
       " ...]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as file:\n",
    "    for item in input_texts:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_len = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_len = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of samples: 10000\n",
      "num of unique input tokens: 71\n",
      "num of unique output tokens: 92\n",
      "max seq len for inputs: 16\n",
      "max seq len for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print('num of samples:', len(input_texts))\n",
    "print('num of unique input tokens:', num_encoder_tokens)\n",
    "print('num of unique output tokens:', num_decoder_tokens)\n",
    "print('max seq len for inputs:', max_encoder_seq_len)\n",
    "print('max seq len for outputs:', max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char,i) for i, char in enumerate(input_characters)]\n",
    ")\n",
    "target_token_index = dict(\n",
    "    [(char,i) for i, char in enumerate(target_characters)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_len, num_encoder_tokens),\n",
    "    dtype = 'float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_len, num_decoder_tokens),    \n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_len, num_decoder_tokens),\n",
    "    dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16, 71)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1\n",
    "    encoder_input_data[i, t+1:, input_token_index[' ']] = 1\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]]=1\n",
    "        if t>0:\n",
    "            decoder_target_data[i, t-1, target_token_index[char]]=1\n",
    "    decoder_input_data[i,t+1:, target_token_index[' ']] = 1\n",
    "    decoder_target_data[i, t:, target_token_index[' ']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 71)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:04:44.862619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9470 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "    encoder = LSTM(latent_dim, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,_,_ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:04:51.363890: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.6931 - loss: 1.6027 - val_accuracy: 0.6972 - val_loss: 1.1523\n",
      "Epoch 2/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7340 - loss: 1.0020 - val_accuracy: 0.7013 - val_loss: 1.1310\n",
      "Epoch 3/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7511 - loss: 0.9047 - val_accuracy: 0.7242 - val_loss: 0.9320\n",
      "Epoch 4/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.7741 - loss: 0.8068 - val_accuracy: 0.7624 - val_loss: 0.8228\n",
      "Epoch 5/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.7950 - loss: 0.7144 - val_accuracy: 0.7763 - val_loss: 0.7713\n",
      "Epoch 6/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8054 - loss: 0.6700 - val_accuracy: 0.7857 - val_loss: 0.7305\n",
      "Epoch 7/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8134 - loss: 0.6360 - val_accuracy: 0.7923 - val_loss: 0.7029\n",
      "Epoch 8/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.8214 - loss: 0.6081 - val_accuracy: 0.8026 - val_loss: 0.6762\n",
      "Epoch 9/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8275 - loss: 0.5902 - val_accuracy: 0.8088 - val_loss: 0.6547\n",
      "Epoch 10/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8348 - loss: 0.5670 - val_accuracy: 0.8113 - val_loss: 0.6433\n",
      "Epoch 11/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8387 - loss: 0.5503 - val_accuracy: 0.8126 - val_loss: 0.6297\n",
      "Epoch 12/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8430 - loss: 0.5360 - val_accuracy: 0.8219 - val_loss: 0.6079\n",
      "Epoch 13/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8487 - loss: 0.5150 - val_accuracy: 0.8263 - val_loss: 0.5897\n",
      "Epoch 14/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8530 - loss: 0.5000 - val_accuracy: 0.8298 - val_loss: 0.5787\n",
      "Epoch 15/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8564 - loss: 0.4889 - val_accuracy: 0.8300 - val_loss: 0.5727\n",
      "Epoch 16/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8589 - loss: 0.4793 - val_accuracy: 0.8359 - val_loss: 0.5558\n",
      "Epoch 17/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8606 - loss: 0.4712 - val_accuracy: 0.8386 - val_loss: 0.5486\n",
      "Epoch 18/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8635 - loss: 0.4607 - val_accuracy: 0.8401 - val_loss: 0.5424\n",
      "Epoch 19/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8659 - loss: 0.4520 - val_accuracy: 0.8425 - val_loss: 0.5324\n",
      "Epoch 20/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8684 - loss: 0.4438 - val_accuracy: 0.8430 - val_loss: 0.5317\n",
      "Epoch 21/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8711 - loss: 0.4351 - val_accuracy: 0.8447 - val_loss: 0.5221\n",
      "Epoch 22/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8728 - loss: 0.4269 - val_accuracy: 0.8463 - val_loss: 0.5176\n",
      "Epoch 23/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8747 - loss: 0.4213 - val_accuracy: 0.8479 - val_loss: 0.5109\n",
      "Epoch 24/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8762 - loss: 0.4150 - val_accuracy: 0.8495 - val_loss: 0.5076\n",
      "Epoch 25/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8790 - loss: 0.4077 - val_accuracy: 0.8505 - val_loss: 0.5026\n",
      "Epoch 26/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8802 - loss: 0.4031 - val_accuracy: 0.8535 - val_loss: 0.4932\n",
      "Epoch 27/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8831 - loss: 0.3911 - val_accuracy: 0.8535 - val_loss: 0.4944\n",
      "Epoch 28/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8846 - loss: 0.3880 - val_accuracy: 0.8553 - val_loss: 0.4902\n",
      "Epoch 29/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8866 - loss: 0.3802 - val_accuracy: 0.8564 - val_loss: 0.4848\n",
      "Epoch 30/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8875 - loss: 0.3778 - val_accuracy: 0.8560 - val_loss: 0.4875\n",
      "Epoch 31/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8890 - loss: 0.3718 - val_accuracy: 0.8582 - val_loss: 0.4809\n",
      "Epoch 32/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8910 - loss: 0.3652 - val_accuracy: 0.8593 - val_loss: 0.4748\n",
      "Epoch 33/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8928 - loss: 0.3601 - val_accuracy: 0.8610 - val_loss: 0.4730\n",
      "Epoch 34/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8940 - loss: 0.3558 - val_accuracy: 0.8600 - val_loss: 0.4732\n",
      "Epoch 35/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8942 - loss: 0.3530 - val_accuracy: 0.8612 - val_loss: 0.4701\n",
      "Epoch 36/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8959 - loss: 0.3489 - val_accuracy: 0.8622 - val_loss: 0.4691\n",
      "Epoch 37/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8965 - loss: 0.3448 - val_accuracy: 0.8623 - val_loss: 0.4668\n",
      "Epoch 38/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8986 - loss: 0.3391 - val_accuracy: 0.8634 - val_loss: 0.4649\n",
      "Epoch 39/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8998 - loss: 0.3351 - val_accuracy: 0.8649 - val_loss: 0.4623\n",
      "Epoch 40/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9011 - loss: 0.3299 - val_accuracy: 0.8644 - val_loss: 0.4614\n",
      "Epoch 41/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9021 - loss: 0.3262 - val_accuracy: 0.8658 - val_loss: 0.4583\n",
      "Epoch 42/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9044 - loss: 0.3207 - val_accuracy: 0.8665 - val_loss: 0.4561\n",
      "Epoch 43/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9040 - loss: 0.3208 - val_accuracy: 0.8654 - val_loss: 0.4608\n",
      "Epoch 44/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9058 - loss: 0.3145 - val_accuracy: 0.8684 - val_loss: 0.4544\n",
      "Epoch 45/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9068 - loss: 0.3102 - val_accuracy: 0.8675 - val_loss: 0.4541\n",
      "Epoch 46/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9089 - loss: 0.3038 - val_accuracy: 0.8668 - val_loss: 0.4567\n",
      "Epoch 47/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9093 - loss: 0.3033 - val_accuracy: 0.8692 - val_loss: 0.4517\n",
      "Epoch 48/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9103 - loss: 0.2992 - val_accuracy: 0.8672 - val_loss: 0.4557\n",
      "Epoch 49/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9118 - loss: 0.2949 - val_accuracy: 0.8685 - val_loss: 0.4530\n",
      "Epoch 50/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9131 - loss: 0.2897 - val_accuracy: 0.8681 - val_loss: 0.4561\n",
      "Epoch 51/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9137 - loss: 0.2869 - val_accuracy: 0.8680 - val_loss: 0.4569\n",
      "Epoch 52/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9148 - loss: 0.2840 - val_accuracy: 0.8692 - val_loss: 0.4568\n",
      "Epoch 53/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9152 - loss: 0.2834 - val_accuracy: 0.8691 - val_loss: 0.4564\n",
      "Epoch 54/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9168 - loss: 0.2781 - val_accuracy: 0.8693 - val_loss: 0.4547\n",
      "Epoch 55/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.9171 - loss: 0.2761 - val_accuracy: 0.8708 - val_loss: 0.4548\n",
      "Epoch 56/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9186 - loss: 0.2711 - val_accuracy: 0.8697 - val_loss: 0.4562\n",
      "Epoch 57/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9197 - loss: 0.2683 - val_accuracy: 0.8706 - val_loss: 0.4551\n",
      "Epoch 58/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9206 - loss: 0.2640 - val_accuracy: 0.8707 - val_loss: 0.4571\n",
      "Epoch 59/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9217 - loss: 0.2622 - val_accuracy: 0.8716 - val_loss: 0.4562\n",
      "Epoch 60/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9231 - loss: 0.2572 - val_accuracy: 0.8692 - val_loss: 0.4610\n",
      "Epoch 61/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9232 - loss: 0.2553 - val_accuracy: 0.8701 - val_loss: 0.4585\n",
      "Epoch 62/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9243 - loss: 0.2511 - val_accuracy: 0.8699 - val_loss: 0.4608\n",
      "Epoch 63/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9253 - loss: 0.2466 - val_accuracy: 0.8714 - val_loss: 0.4594\n",
      "Epoch 64/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9265 - loss: 0.2447 - val_accuracy: 0.8712 - val_loss: 0.4595\n",
      "Epoch 65/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9266 - loss: 0.2451 - val_accuracy: 0.8720 - val_loss: 0.4586\n",
      "Epoch 66/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9273 - loss: 0.2412 - val_accuracy: 0.8705 - val_loss: 0.4640\n",
      "Epoch 67/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9275 - loss: 0.2396 - val_accuracy: 0.8712 - val_loss: 0.4642\n",
      "Epoch 68/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9284 - loss: 0.2384 - val_accuracy: 0.8700 - val_loss: 0.4682\n",
      "Epoch 69/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9303 - loss: 0.2315 - val_accuracy: 0.8712 - val_loss: 0.4667\n",
      "Epoch 70/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9319 - loss: 0.2282 - val_accuracy: 0.8717 - val_loss: 0.4676\n",
      "Epoch 71/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9322 - loss: 0.2252 - val_accuracy: 0.8720 - val_loss: 0.4694\n",
      "Epoch 72/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9315 - loss: 0.2253 - val_accuracy: 0.8706 - val_loss: 0.4688\n",
      "Epoch 73/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9331 - loss: 0.2215 - val_accuracy: 0.8710 - val_loss: 0.4711\n",
      "Epoch 74/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9332 - loss: 0.2209 - val_accuracy: 0.8706 - val_loss: 0.4772\n",
      "Epoch 75/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9347 - loss: 0.2155 - val_accuracy: 0.8707 - val_loss: 0.4765\n",
      "Epoch 76/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9352 - loss: 0.2143 - val_accuracy: 0.8711 - val_loss: 0.4778\n",
      "Epoch 77/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9367 - loss: 0.2100 - val_accuracy: 0.8708 - val_loss: 0.4803\n",
      "Epoch 78/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9371 - loss: 0.2084 - val_accuracy: 0.8707 - val_loss: 0.4826\n",
      "Epoch 79/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9378 - loss: 0.2055 - val_accuracy: 0.8715 - val_loss: 0.4795\n",
      "Epoch 80/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9393 - loss: 0.2008 - val_accuracy: 0.8714 - val_loss: 0.4828\n",
      "Epoch 81/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9384 - loss: 0.2031 - val_accuracy: 0.8701 - val_loss: 0.4889\n",
      "Epoch 82/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9400 - loss: 0.1980 - val_accuracy: 0.8714 - val_loss: 0.4862\n",
      "Epoch 83/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9410 - loss: 0.1960 - val_accuracy: 0.8702 - val_loss: 0.4935\n",
      "Epoch 84/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9418 - loss: 0.1934 - val_accuracy: 0.8704 - val_loss: 0.4928\n",
      "Epoch 85/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9419 - loss: 0.1917 - val_accuracy: 0.8712 - val_loss: 0.4917\n",
      "Epoch 86/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9419 - loss: 0.1908 - val_accuracy: 0.8719 - val_loss: 0.4921\n",
      "Epoch 87/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9431 - loss: 0.1887 - val_accuracy: 0.8714 - val_loss: 0.4968\n",
      "Epoch 88/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9433 - loss: 0.1862 - val_accuracy: 0.8699 - val_loss: 0.5028\n",
      "Epoch 89/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9443 - loss: 0.1843 - val_accuracy: 0.8711 - val_loss: 0.5000\n",
      "Epoch 90/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9449 - loss: 0.1806 - val_accuracy: 0.8707 - val_loss: 0.5042\n",
      "Epoch 91/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9455 - loss: 0.1792 - val_accuracy: 0.8703 - val_loss: 0.5073\n",
      "Epoch 92/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9466 - loss: 0.1782 - val_accuracy: 0.8704 - val_loss: 0.5089\n",
      "Epoch 93/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9471 - loss: 0.1743 - val_accuracy: 0.8702 - val_loss: 0.5125\n",
      "Epoch 94/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9472 - loss: 0.1744 - val_accuracy: 0.8703 - val_loss: 0.5102\n",
      "Epoch 95/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9479 - loss: 0.1716 - val_accuracy: 0.8698 - val_loss: 0.5206\n",
      "Epoch 96/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9480 - loss: 0.1711 - val_accuracy: 0.8704 - val_loss: 0.5165\n",
      "Epoch 97/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9494 - loss: 0.1661 - val_accuracy: 0.8703 - val_loss: 0.5196\n",
      "Epoch 98/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9497 - loss: 0.1648 - val_accuracy: 0.8690 - val_loss: 0.5252\n",
      "Epoch 99/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9503 - loss: 0.1647 - val_accuracy: 0.8701 - val_loss: 0.5239\n",
      "Epoch 100/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9500 - loss: 0.1632 - val_accuracy: 0.8700 - val_loss: 0.5287\n",
      "Epoch 101/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9515 - loss: 0.1595 - val_accuracy: 0.8703 - val_loss: 0.5280\n",
      "Epoch 102/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9518 - loss: 0.1580 - val_accuracy: 0.8706 - val_loss: 0.5262\n",
      "Epoch 103/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9532 - loss: 0.1552 - val_accuracy: 0.8703 - val_loss: 0.5344\n",
      "Epoch 104/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9528 - loss: 0.1555 - val_accuracy: 0.8693 - val_loss: 0.5363\n",
      "Epoch 105/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9531 - loss: 0.1535 - val_accuracy: 0.8703 - val_loss: 0.5397\n",
      "Epoch 106/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9535 - loss: 0.1519 - val_accuracy: 0.8702 - val_loss: 0.5372\n",
      "Epoch 107/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9543 - loss: 0.1505 - val_accuracy: 0.8693 - val_loss: 0.5465\n",
      "Epoch 108/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9546 - loss: 0.1490 - val_accuracy: 0.8701 - val_loss: 0.5463\n",
      "Epoch 109/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9557 - loss: 0.1452 - val_accuracy: 0.8696 - val_loss: 0.5458\n",
      "Epoch 110/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9560 - loss: 0.1445 - val_accuracy: 0.8701 - val_loss: 0.5492\n",
      "Epoch 111/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9564 - loss: 0.1429 - val_accuracy: 0.8695 - val_loss: 0.5510\n",
      "Epoch 112/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9565 - loss: 0.1420 - val_accuracy: 0.8692 - val_loss: 0.5543\n",
      "Epoch 113/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9570 - loss: 0.1410 - val_accuracy: 0.8696 - val_loss: 0.5567\n",
      "Epoch 114/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9573 - loss: 0.1392 - val_accuracy: 0.8694 - val_loss: 0.5606\n",
      "Epoch 115/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9581 - loss: 0.1369 - val_accuracy: 0.8699 - val_loss: 0.5620\n",
      "Epoch 116/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9584 - loss: 0.1363 - val_accuracy: 0.8701 - val_loss: 0.5629\n",
      "Epoch 117/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9587 - loss: 0.1339 - val_accuracy: 0.8690 - val_loss: 0.5697\n",
      "Epoch 118/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9592 - loss: 0.1333 - val_accuracy: 0.8685 - val_loss: 0.5727\n",
      "Epoch 119/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9596 - loss: 0.1327 - val_accuracy: 0.8696 - val_loss: 0.5700\n",
      "Epoch 120/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9602 - loss: 0.1298 - val_accuracy: 0.8683 - val_loss: 0.5752\n",
      "Epoch 121/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9604 - loss: 0.1284 - val_accuracy: 0.8694 - val_loss: 0.5735\n",
      "Epoch 122/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9608 - loss: 0.1282 - val_accuracy: 0.8688 - val_loss: 0.5803\n",
      "Epoch 123/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9617 - loss: 0.1254 - val_accuracy: 0.8671 - val_loss: 0.5859\n",
      "Epoch 124/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9617 - loss: 0.1253 - val_accuracy: 0.8693 - val_loss: 0.5820\n",
      "Epoch 125/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9623 - loss: 0.1232 - val_accuracy: 0.8697 - val_loss: 0.5855\n",
      "Epoch 126/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9623 - loss: 0.1220 - val_accuracy: 0.8684 - val_loss: 0.5904\n",
      "Epoch 127/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9627 - loss: 0.1213 - val_accuracy: 0.8688 - val_loss: 0.5940\n",
      "Epoch 128/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9633 - loss: 0.1199 - val_accuracy: 0.8680 - val_loss: 0.5954\n",
      "Epoch 129/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9632 - loss: 0.1193 - val_accuracy: 0.8688 - val_loss: 0.5990\n",
      "Epoch 130/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9633 - loss: 0.1189 - val_accuracy: 0.8678 - val_loss: 0.5995\n",
      "Epoch 131/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9637 - loss: 0.1172 - val_accuracy: 0.8677 - val_loss: 0.6011\n",
      "Epoch 132/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9643 - loss: 0.1161 - val_accuracy: 0.8683 - val_loss: 0.6021\n",
      "Epoch 133/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9650 - loss: 0.1138 - val_accuracy: 0.8685 - val_loss: 0.6084\n",
      "Epoch 134/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9651 - loss: 0.1139 - val_accuracy: 0.8686 - val_loss: 0.6059\n",
      "Epoch 135/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9654 - loss: 0.1122 - val_accuracy: 0.8676 - val_loss: 0.6133\n",
      "Epoch 136/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9658 - loss: 0.1114 - val_accuracy: 0.8695 - val_loss: 0.6095\n",
      "Epoch 137/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9660 - loss: 0.1101 - val_accuracy: 0.8674 - val_loss: 0.6221\n",
      "Epoch 138/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9663 - loss: 0.1088 - val_accuracy: 0.8679 - val_loss: 0.6160\n",
      "Epoch 139/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9665 - loss: 0.1082 - val_accuracy: 0.8680 - val_loss: 0.6178\n",
      "Epoch 140/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9674 - loss: 0.1058 - val_accuracy: 0.8681 - val_loss: 0.6225\n",
      "Epoch 141/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9676 - loss: 0.1048 - val_accuracy: 0.8685 - val_loss: 0.6247\n",
      "Epoch 142/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9675 - loss: 0.1053 - val_accuracy: 0.8679 - val_loss: 0.6311\n",
      "Epoch 143/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9677 - loss: 0.1038 - val_accuracy: 0.8678 - val_loss: 0.6309\n",
      "Epoch 144/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9681 - loss: 0.1031 - val_accuracy: 0.8681 - val_loss: 0.6311\n",
      "Epoch 145/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9689 - loss: 0.1010 - val_accuracy: 0.8677 - val_loss: 0.6346\n",
      "Epoch 146/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9687 - loss: 0.1014 - val_accuracy: 0.8674 - val_loss: 0.6368\n",
      "Epoch 147/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9691 - loss: 0.0990 - val_accuracy: 0.8678 - val_loss: 0.6435\n",
      "Epoch 148/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9694 - loss: 0.0985 - val_accuracy: 0.8676 - val_loss: 0.6410\n",
      "Epoch 149/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9691 - loss: 0.0986 - val_accuracy: 0.8668 - val_loss: 0.6488\n",
      "Epoch 150/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9699 - loss: 0.0968 - val_accuracy: 0.8673 - val_loss: 0.6463\n",
      "Epoch 151/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9699 - loss: 0.0967 - val_accuracy: 0.8672 - val_loss: 0.6492\n",
      "Epoch 152/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9708 - loss: 0.0946 - val_accuracy: 0.8675 - val_loss: 0.6548\n",
      "Epoch 153/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9706 - loss: 0.0948 - val_accuracy: 0.8663 - val_loss: 0.6580\n",
      "Epoch 154/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9708 - loss: 0.0933 - val_accuracy: 0.8673 - val_loss: 0.6560\n",
      "Epoch 155/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9710 - loss: 0.0922 - val_accuracy: 0.8672 - val_loss: 0.6594\n",
      "Epoch 156/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9715 - loss: 0.0914 - val_accuracy: 0.8670 - val_loss: 0.6620\n",
      "Epoch 157/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9715 - loss: 0.0912 - val_accuracy: 0.8681 - val_loss: 0.6589\n",
      "Epoch 158/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9722 - loss: 0.0897 - val_accuracy: 0.8664 - val_loss: 0.6691\n",
      "Epoch 159/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9726 - loss: 0.0882 - val_accuracy: 0.8676 - val_loss: 0.6615\n",
      "Epoch 160/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9724 - loss: 0.0885 - val_accuracy: 0.8670 - val_loss: 0.6688\n",
      "Epoch 161/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9725 - loss: 0.0871 - val_accuracy: 0.8670 - val_loss: 0.6741\n",
      "Epoch 162/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9727 - loss: 0.0869 - val_accuracy: 0.8659 - val_loss: 0.6774\n",
      "Epoch 163/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9728 - loss: 0.0861 - val_accuracy: 0.8670 - val_loss: 0.6773\n",
      "Epoch 164/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9734 - loss: 0.0851 - val_accuracy: 0.8606 - val_loss: 0.7057\n",
      "Epoch 165/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9725 - loss: 0.0873 - val_accuracy: 0.8662 - val_loss: 0.6807\n",
      "Epoch 166/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9742 - loss: 0.0821 - val_accuracy: 0.8660 - val_loss: 0.6849\n",
      "Epoch 167/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9741 - loss: 0.0825 - val_accuracy: 0.8662 - val_loss: 0.6899\n",
      "Epoch 168/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9745 - loss: 0.0818 - val_accuracy: 0.8669 - val_loss: 0.6856\n",
      "Epoch 169/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9746 - loss: 0.0814 - val_accuracy: 0.8659 - val_loss: 0.6948\n",
      "Epoch 170/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9747 - loss: 0.0813 - val_accuracy: 0.8666 - val_loss: 0.6950\n",
      "Epoch 171/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9750 - loss: 0.0788 - val_accuracy: 0.8663 - val_loss: 0.7018\n",
      "Epoch 172/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9749 - loss: 0.0799 - val_accuracy: 0.8666 - val_loss: 0.6984\n",
      "Epoch 173/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9758 - loss: 0.0777 - val_accuracy: 0.8662 - val_loss: 0.7046\n",
      "Epoch 174/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9751 - loss: 0.0779 - val_accuracy: 0.8654 - val_loss: 0.7032\n",
      "Epoch 175/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9754 - loss: 0.0767 - val_accuracy: 0.8669 - val_loss: 0.7058\n",
      "Epoch 176/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9762 - loss: 0.0754 - val_accuracy: 0.8664 - val_loss: 0.7101\n",
      "Epoch 177/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9760 - loss: 0.0757 - val_accuracy: 0.8656 - val_loss: 0.7106\n",
      "Epoch 178/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9763 - loss: 0.0748 - val_accuracy: 0.8657 - val_loss: 0.7187\n",
      "Epoch 179/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9763 - loss: 0.0740 - val_accuracy: 0.8669 - val_loss: 0.7096\n",
      "Epoch 180/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9767 - loss: 0.0732 - val_accuracy: 0.8661 - val_loss: 0.7165\n",
      "Epoch 181/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9768 - loss: 0.0732 - val_accuracy: 0.8659 - val_loss: 0.7159\n",
      "Epoch 182/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9775 - loss: 0.0716 - val_accuracy: 0.8659 - val_loss: 0.7198\n",
      "Epoch 183/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9771 - loss: 0.0718 - val_accuracy: 0.8663 - val_loss: 0.7224\n",
      "Epoch 184/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9775 - loss: 0.0705 - val_accuracy: 0.8656 - val_loss: 0.7237\n",
      "Epoch 185/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9779 - loss: 0.0696 - val_accuracy: 0.8650 - val_loss: 0.7279\n",
      "Epoch 186/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9779 - loss: 0.0692 - val_accuracy: 0.8666 - val_loss: 0.7299\n",
      "Epoch 187/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9780 - loss: 0.0686 - val_accuracy: 0.8652 - val_loss: 0.7375\n",
      "Epoch 188/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9781 - loss: 0.0693 - val_accuracy: 0.8658 - val_loss: 0.7360\n",
      "Epoch 189/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9779 - loss: 0.0688 - val_accuracy: 0.8650 - val_loss: 0.7398\n",
      "Epoch 190/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9785 - loss: 0.0679 - val_accuracy: 0.8657 - val_loss: 0.7368\n",
      "Epoch 191/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9789 - loss: 0.0663 - val_accuracy: 0.8651 - val_loss: 0.7388\n",
      "Epoch 192/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9789 - loss: 0.0658 - val_accuracy: 0.8649 - val_loss: 0.7459\n",
      "Epoch 193/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9791 - loss: 0.0653 - val_accuracy: 0.8655 - val_loss: 0.7479\n",
      "Epoch 194/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9792 - loss: 0.0648 - val_accuracy: 0.8648 - val_loss: 0.7470\n",
      "Epoch 195/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9795 - loss: 0.0638 - val_accuracy: 0.8655 - val_loss: 0.7479\n",
      "Epoch 196/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9793 - loss: 0.0641 - val_accuracy: 0.8643 - val_loss: 0.7578\n",
      "Epoch 197/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9797 - loss: 0.0631 - val_accuracy: 0.8634 - val_loss: 0.7617\n",
      "Epoch 198/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9793 - loss: 0.0642 - val_accuracy: 0.8644 - val_loss: 0.7597\n",
      "Epoch 199/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9800 - loss: 0.0622 - val_accuracy: 0.8661 - val_loss: 0.7563\n",
      "Epoch 200/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9800 - loss: 0.0616 - val_accuracy: 0.8657 - val_loss: 0.7563\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x797fa5cbe6b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, \n",
    "         batch_size = batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state= decoder_states_inputs)\n",
    "decoder_states=[state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs]+decoder_states_inputs, [decoder_outputs]+decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index= dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index= dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence  : Go.\n",
      "Decoded Sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence  : Hi.\n",
      "Decoded Sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence  : Hi.\n",
      "Decoded Sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence  : Run!\n",
      "Decoded Sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence  : Run!\n",
      "Decoded Sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence  : Who?\n",
      "Decoded Sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence  : Wow!\n",
      "Decoded Sentence: Lave-toi !\n",
      "\n",
      "-\n",
      "Input sentence  : Fire!\n",
      "Decoded Sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence  : Help!\n",
      "Decoded Sentence: Aide-moi.\n",
      "\n",
      "-\n",
      "Input sentence  : Jump.\n",
      "Decoded Sentence: Maute-moi.\n",
      "\n",
      "-\n",
      "Input sentence  : Stop!\n",
      "Decoded Sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence  : Stop!\n",
      "Decoded Sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence  : Stop!\n",
      "Decoded Sentence: Stop !\n",
      "\n",
      "-\n",
      "Input sentence  : Wait!\n",
      "Decoded Sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence  : Wait!\n",
      "Decoded Sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence  : Go on.\n",
      "Decoded Sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Go on.\n",
      "Decoded Sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Go on.\n",
      "Decoded Sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Hello!\n",
      "Decoded Sentence: Aide-moi !\n",
      "\n",
      "-\n",
      "Input sentence  : Hello!\n",
      "Decoded Sentence: Aide-moi !\n",
      "\n",
      "-\n",
      "Input sentence  : I see.\n",
      "Decoded Sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence  : I try.\n",
      "Decoded Sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence  : I won!\n",
      "Decoded Sentence: J'ai gagnée.\n",
      "\n",
      "-\n",
      "Input sentence  : I won!\n",
      "Decoded Sentence: J'ai gagnée.\n",
      "\n",
      "-\n",
      "Input sentence  : I won.\n",
      "Decoded Sentence: J'ai gagné.\n",
      "\n",
      "-\n",
      "Input sentence  : Oh no!\n",
      "Decoded Sentence: Essayez !\n",
      "\n",
      "-\n",
      "Input sentence  : Attack!\n",
      "Decoded Sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence  : Attack!\n",
      "Decoded Sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: À votre santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Get up.\n",
      "Decoded Sentence: Sors.\n",
      "\n",
      "-\n",
      "Input sentence  : Go now.\n",
      "Decoded Sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Go now.\n",
      "Decoded Sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Go now.\n",
      "Decoded Sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Got it!\n",
      "Decoded Sentence: Allez-vous en !\n",
      "\n",
      "-\n",
      "Input sentence  : Got it!\n",
      "Decoded Sentence: Allez-vous en !\n",
      "\n",
      "-\n",
      "Input sentence  : Got it?\n",
      "Decoded Sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence  : Got it?\n",
      "Decoded Sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence  : Got it?\n",
      "Decoded Sentence: Pigé ?\n",
      "\n",
      "-\n",
      "Input sentence  : Hop in.\n",
      "Decoded Sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence  : Hop in.\n",
      "Decoded Sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence  : Hug me.\n",
      "Decoded Sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence  : Hug me.\n",
      "Decoded Sentence: Serre-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence  : I fell.\n",
      "Decoded Sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input sentence  : I fell.\n",
      "Decoded Sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input sentence  : I know.\n",
      "Decoded Sentence: Je le soupi.\n",
      "\n",
      "-\n",
      "Input sentence  : I left.\n",
      "Decoded Sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input sentence  : I left.\n",
      "Decoded Sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input sentence  : I lied.\n",
      "Decoded Sentence: J'ai menti.\n",
      "\n",
      "-\n",
      "Input sentence  : I lost.\n",
      "Decoded Sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence  : I paid.\n",
      "Decoded Sentence: J’ai payé.\n",
      "\n",
      "-\n",
      "Input sentence  : I'm 19.\n",
      "Decoded Sentence: Je suis oncupée.\n",
      "\n",
      "-\n",
      "Input sentence  : I'm OK.\n",
      "Decoded Sentence: Je suis oncupée.\n",
      "\n",
      "-\n",
      "Input sentence  : I'm OK.\n",
      "Decoded Sentence: Je suis oncupée.\n",
      "\n",
      "-\n",
      "Input sentence  : Listen.\n",
      "Decoded Sentence: Achate-toi.\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est hors de question !\n",
      "\n",
      "-\n",
      "Input sentence  : Really?\n",
      "Decoded Sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence  : Really?\n",
      "Decoded Sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence  : Really?\n",
      "Decoded Sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence  : Thanks.\n",
      "Decoded Sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence  : We try.\n",
      "Decoded Sentence: Nous nous entendons.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'avons emporté.\n",
      "\n",
      "-\n",
      "Input sentence  : Ask Tom.\n",
      "Decoded Sentence: Demande-leur.\n",
      "\n",
      "-\n",
      "Input sentence  : Awesome!\n",
      "Decoded Sentence: Allez domme-toi.\n",
      "\n",
      "-\n",
      "Input sentence  : Be calm.\n",
      "Decoded Sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence  : Be calm.\n",
      "Decoded Sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence  : Be calm.\n",
      "Decoded Sentence: Soyez calme !\n",
      "\n",
      "-\n",
      "Input sentence  : Be cool.\n",
      "Decoded Sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Sois juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Sois juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Sois juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Sois juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Sois juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Sois juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be kind.\n",
      "Decoded Sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Beat it.\n",
      "Decoded Sentence: Aucune idée.\n",
      "\n",
      "-\n",
      "Input sentence  : Call me.\n",
      "Decoded Sentence: Appelez-moi !\n",
      "\n",
      "-\n",
      "Input sentence  : Call me.\n",
      "Decoded Sentence: Appelez-moi !\n",
      "\n",
      "-\n",
      "Input sentence  : Call us.\n",
      "Decoded Sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence  : Call us.\n",
      "Decoded Sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence  : Come in.\n",
      "Decoded Sentence: Entrez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    \n",
    "    target_seq[0, 0, target_token_index['\\t']]=1.\n",
    "    \n",
    "    stop_condition= False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h,c = decoder_model.predict([target_seq]+states_value, verbose=0)\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "       \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += sampled_char\n",
    "        if (sampled_char=='\\n' or len(decoded_sentence)>max_decoder_seq_len):\n",
    "            stop_condition=True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0,0, sampled_token_index]=1.\n",
    "        \n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decode_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence  :', input_texts[seq_index])\n",
    "    print('Decoded Sentence:', decode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 802062,
     "sourceId": 1375271,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30197,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
