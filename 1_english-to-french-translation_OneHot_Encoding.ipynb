{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 12:56:13.836937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-28 12:56:13.853231: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-28 12:56:13.858143: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-28 12:56:13.871113: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 12:56:14.751290: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 200\n",
    "latent_dim = 256\n",
    "num_samples = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset/fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path,'r',encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:min(num_samples,len(lines)-1)]:\n",
    "    input_text, target_text,_= line.split('\\t')\n",
    "    target_text = '\\t'+target_text+'\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.',\n",
       " 'Hi.',\n",
       " 'Hi.',\n",
       " 'Run!',\n",
       " 'Run!',\n",
       " 'Who?',\n",
       " 'Wow!',\n",
       " 'Fire!',\n",
       " 'Help!',\n",
       " 'Jump.',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Stop!',\n",
       " 'Wait!',\n",
       " 'Wait!',\n",
       " 'Go on.',\n",
       " 'Go on.',\n",
       " 'Go on.',\n",
       " 'Hello!',\n",
       " 'Hello!',\n",
       " 'I see.',\n",
       " 'I try.',\n",
       " 'I won!',\n",
       " 'I won!',\n",
       " 'I won.',\n",
       " 'Oh no!',\n",
       " 'Attack!',\n",
       " 'Attack!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Cheers!',\n",
       " 'Get up.',\n",
       " 'Go now.',\n",
       " 'Go now.',\n",
       " 'Go now.',\n",
       " 'Got it!',\n",
       " 'Got it!',\n",
       " 'Got it?',\n",
       " 'Got it?',\n",
       " 'Got it?',\n",
       " 'Hop in.',\n",
       " 'Hop in.',\n",
       " 'Hug me.',\n",
       " 'Hug me.',\n",
       " 'I fell.',\n",
       " 'I fell.',\n",
       " 'I know.',\n",
       " 'I left.',\n",
       " 'I left.',\n",
       " 'I lied.',\n",
       " 'I lost.',\n",
       " 'I paid.',\n",
       " \"I'm 19.\",\n",
       " \"I'm OK.\",\n",
       " \"I'm OK.\",\n",
       " 'Listen.',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'No way!',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Really?',\n",
       " 'Thanks.',\n",
       " 'We try.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'We won.',\n",
       " 'Ask Tom.',\n",
       " 'Awesome!',\n",
       " 'Be calm.',\n",
       " 'Be calm.',\n",
       " 'Be calm.',\n",
       " 'Be cool.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be fair.',\n",
       " 'Be kind.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Be nice.',\n",
       " 'Beat it.',\n",
       " 'Call me.',\n",
       " 'Call me.',\n",
       " 'Call us.',\n",
       " 'Call us.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come in.',\n",
       " 'Come on!',\n",
       " 'Come on.',\n",
       " 'Come on.',\n",
       " 'Come on.',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Drop it!',\n",
       " 'Get Tom.',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out!',\n",
       " 'Get out.',\n",
       " 'Get out.',\n",
       " 'Go away!',\n",
       " 'Go away!',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go away.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go home.',\n",
       " 'Go slow.',\n",
       " 'Go slow.',\n",
       " 'Goodbye!',\n",
       " 'Goodbye!',\n",
       " 'Hang on!',\n",
       " 'Hang on!',\n",
       " 'Hang on.',\n",
       " 'Hang on.',\n",
       " 'He quit.',\n",
       " 'He quit.',\n",
       " 'He runs.',\n",
       " 'Help me!',\n",
       " 'Help me.',\n",
       " 'Help me.',\n",
       " 'Help us.',\n",
       " 'Help us.',\n",
       " 'Hold it!',\n",
       " 'Hold on.',\n",
       " 'Hug Tom.',\n",
       " 'I agree.',\n",
       " 'I cried.',\n",
       " 'I dozed.',\n",
       " 'I dozed.',\n",
       " 'I drive.',\n",
       " 'I smoke.',\n",
       " 'I snore.',\n",
       " 'I stink.',\n",
       " 'I stood.',\n",
       " 'I stood.',\n",
       " 'I swore.',\n",
       " 'I swore.',\n",
       " 'I tried.',\n",
       " 'I tried.',\n",
       " 'I tried.',\n",
       " 'I waved.',\n",
       " \"I'll go.\",\n",
       " \"I'm Tom.\",\n",
       " \"I'm fat.\",\n",
       " \"I'm fat.\",\n",
       " \"I'm fit.\",\n",
       " \"I'm hit!\",\n",
       " \"I'm hit!\",\n",
       " \"I'm ill.\",\n",
       " \"I'm sad.\",\n",
       " \"I'm shy.\",\n",
       " \"I'm wet.\",\n",
       " \"I'm wet.\",\n",
       " \"It's me!\",\n",
       " 'Join us.',\n",
       " 'Join us.',\n",
       " 'Keep it.',\n",
       " 'Keep it.',\n",
       " 'Kiss me.',\n",
       " 'Kiss me.',\n",
       " 'Me, too.',\n",
       " 'Open up.',\n",
       " 'Open up.',\n",
       " 'Perfect!',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you!',\n",
       " 'See you.',\n",
       " 'Show me.',\n",
       " 'Show me.',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Shut up!',\n",
       " 'Skip it.',\n",
       " 'So long.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Take it.',\n",
       " 'Tell me.',\n",
       " 'Tell me.',\n",
       " 'Tom won.',\n",
       " 'Wake up!',\n",
       " 'Wake up!',\n",
       " 'Wake up!',\n",
       " 'Wake up.',\n",
       " 'Wake up.',\n",
       " 'Wash up.',\n",
       " 'Wash up.',\n",
       " 'We know.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'We lost.',\n",
       " 'Welcome.',\n",
       " 'Who won?',\n",
       " 'Who won?',\n",
       " 'You run.',\n",
       " 'Am I fat?',\n",
       " 'Am I fat?',\n",
       " 'Ask them.',\n",
       " 'Ask them.',\n",
       " 'Back off!',\n",
       " 'Back off!',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Back off.',\n",
       " 'Be a man.',\n",
       " 'Be a man.',\n",
       " 'Be still.',\n",
       " 'Be still.',\n",
       " 'Be still.',\n",
       " 'Beats me.',\n",
       " 'Beats me.',\n",
       " 'Call Tom.',\n",
       " 'Call Tom.',\n",
       " 'Cheer up!',\n",
       " 'Cool off!',\n",
       " 'Cuff him.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Drive on.',\n",
       " 'Find Tom.',\n",
       " 'Find Tom.',\n",
       " 'Fix this.',\n",
       " 'Fix this.',\n",
       " 'Get down!',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get down.',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get lost!',\n",
       " 'Get real!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead!',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Go ahead.',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Good job!',\n",
       " 'Grab him.',\n",
       " 'Grab him.',\n",
       " 'Have fun.',\n",
       " 'Have fun.',\n",
       " 'He tries.',\n",
       " \"He's wet.\",\n",
       " 'Help Tom.',\n",
       " 'Help Tom.',\n",
       " 'Hi, guys.',\n",
       " 'How cute!',\n",
       " 'How deep?',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'How nice!',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'Hurry up.',\n",
       " 'I did OK.',\n",
       " 'I did OK.',\n",
       " 'I did it.',\n",
       " 'I did it.',\n",
       " 'I failed.',\n",
       " 'I forgot.',\n",
       " 'I get it.',\n",
       " 'I got it.',\n",
       " 'I got it.',\n",
       " 'I helped.',\n",
       " 'I jumped.',\n",
       " 'I looked.',\n",
       " 'I moaned.',\n",
       " 'I nodded.',\n",
       " 'I obeyed.',\n",
       " 'I phoned.',\n",
       " 'I phoned.',\n",
       " 'I refuse.',\n",
       " 'I refuse.',\n",
       " 'I rested.',\n",
       " 'I rested.',\n",
       " 'I saw it.',\n",
       " 'I saw it.',\n",
       " 'I sighed.',\n",
       " 'I stayed.',\n",
       " 'I stayed.',\n",
       " 'I talked.',\n",
       " 'I use it.',\n",
       " 'I use it.',\n",
       " 'I use it.',\n",
       " \"I'll pay.\",\n",
       " \"I'll pay.\",\n",
       " \"I'll try.\",\n",
       " \"I'll try.\",\n",
       " \"I'm back.\",\n",
       " \"I'm back.\",\n",
       " \"I'm bald.\",\n",
       " \"I'm busy.\",\n",
       " \"I'm busy.\",\n",
       " \"I'm calm.\",\n",
       " \"I'm cold.\",\n",
       " \"I'm cool.\",\n",
       " \"I'm cool.\",\n",
       " \"I'm deaf.\",\n",
       " \"I'm deaf.\",\n",
       " \"I'm done.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fair.\",\n",
       " \"I'm fast.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm fine.\",\n",
       " \"I'm free!\",\n",
       " \"I'm free.\",\n",
       " \"I'm free.\",\n",
       " \"I'm full.\",\n",
       " \"I'm full.\",\n",
       " \"I'm game.\",\n",
       " \"I'm game.\",\n",
       " \"I'm glad.\",\n",
       " \"I'm home.\",\n",
       " \"I'm late.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm lazy.\",\n",
       " \"I'm okay.\",\n",
       " \"I'm okay.\",\n",
       " \"I'm rich.\",\n",
       " \"I'm safe.\",\n",
       " \"I'm sick.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm sure.\",\n",
       " \"I'm tall.\",\n",
       " \"I'm thin.\",\n",
       " \"I'm tidy.\",\n",
       " \"I'm tidy.\",\n",
       " \"I'm ugly.\",\n",
       " \"I'm ugly.\",\n",
       " \"I'm weak.\",\n",
       " \"I'm well.\",\n",
       " \"I'm well.\",\n",
       " \"I've won.\",\n",
       " \"I've won.\",\n",
       " 'It helps.',\n",
       " 'It hurts.',\n",
       " 'It works.',\n",
       " 'It works.',\n",
       " \"It's Tom.\",\n",
       " \"It's fun.\",\n",
       " \"It's fun.\",\n",
       " \"It's his.\",\n",
       " \"It's his.\",\n",
       " \"It's new.\",\n",
       " \"It's new.\",\n",
       " \"It's odd.\",\n",
       " \"It's red.\",\n",
       " \"It's sad.\",\n",
       " 'Keep out!',\n",
       " 'Keep out.',\n",
       " 'Kiss Tom.',\n",
       " 'Leave it.',\n",
       " 'Leave it.',\n",
       " 'Leave me.',\n",
       " 'Leave us.',\n",
       " 'Leave us.',\n",
       " \"Let's go!\",\n",
       " \"Let's go.\",\n",
       " 'Look out!',\n",
       " 'Look out!',\n",
       " 'Marry me.',\n",
       " 'Marry me.',\n",
       " 'May I go?',\n",
       " 'May I go?',\n",
       " 'May I go?',\n",
       " 'Save Tom.',\n",
       " 'Save Tom.',\n",
       " 'Say what?',\n",
       " 'She came.',\n",
       " 'She died.',\n",
       " 'She runs.',\n",
       " 'Sit down!',\n",
       " 'Sit down!',\n",
       " 'Sit down.',\n",
       " 'Sit here.',\n",
       " 'Sit here.',\n",
       " 'Speak up!',\n",
       " 'Speak up!',\n",
       " 'Stand up.',\n",
       " 'Stop Tom.',\n",
       " 'Stop Tom.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Taste it.',\n",
       " 'Tell Tom.',\n",
       " 'Tell Tom.',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'Terrific!',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'They won.',\n",
       " 'Tom came.',\n",
       " 'Tom died.',\n",
       " 'Tom knew.',\n",
       " 'Tom left.',\n",
       " 'Tom left.',\n",
       " 'Tom lied.',\n",
       " 'Tom lies.',\n",
       " 'Tom lost.',\n",
       " 'Tom paid.',\n",
       " 'Too late.',\n",
       " 'Trust me.',\n",
       " 'Trust me.',\n",
       " 'Try hard.',\n",
       " 'Try some.',\n",
       " 'Try some.',\n",
       " 'Try this.',\n",
       " 'Try this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Use this.',\n",
       " 'Warn Tom.',\n",
       " 'Warn Tom.',\n",
       " 'Watch me.',\n",
       " 'Watch me.',\n",
       " 'Watch us.',\n",
       " 'Watch us.',\n",
       " 'We agree.',\n",
       " \"We'll go.\",\n",
       " \"We're OK.\",\n",
       " 'What for?',\n",
       " 'What for?',\n",
       " 'What fun!',\n",
       " 'What fun!',\n",
       " 'Who came?',\n",
       " 'Who died?',\n",
       " 'Who fell?',\n",
       " 'Who lost?',\n",
       " 'Who quit?',\n",
       " \"Who's he?\",\n",
       " 'Write me.',\n",
       " 'Write me.',\n",
       " 'You lost.',\n",
       " 'You lost.',\n",
       " 'After you.',\n",
       " 'Aim. Fire!',\n",
       " 'Am I late?',\n",
       " 'Answer me.',\n",
       " 'Be seated.',\n",
       " 'Be seated.',\n",
       " 'Birds fly.',\n",
       " 'Bless you.',\n",
       " 'Call home!',\n",
       " 'Calm down!',\n",
       " 'Calm down.',\n",
       " 'Can we go?',\n",
       " 'Can we go?',\n",
       " 'Can we go?',\n",
       " 'Catch Tom.',\n",
       " 'Catch Tom.',\n",
       " 'Catch him.',\n",
       " 'Chill out.',\n",
       " 'Come back.',\n",
       " 'Come back.',\n",
       " 'Come here.',\n",
       " 'Come here.',\n",
       " 'Come over!',\n",
       " 'Come over!',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come over.',\n",
       " 'Come soon.',\n",
       " 'Come soon.',\n",
       " 'Cool down.',\n",
       " 'Did I win?',\n",
       " 'Did I win?',\n",
       " 'Did I win?',\n",
       " 'Do it now.',\n",
       " 'Dogs bark.',\n",
       " 'Dogs bark.',\n",
       " \"Don't ask.\",\n",
       " \"Don't cry.\",\n",
       " \"Don't die.\",\n",
       " \"Don't die.\",\n",
       " \"Don't lie.\",\n",
       " \"Don't run.\",\n",
       " \"Don't run.\",\n",
       " 'Excuse me.',\n",
       " 'Excuse me.',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Excuse me?',\n",
       " 'Fantastic!',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Feel this.',\n",
       " 'Follow me.',\n",
       " 'Follow us.',\n",
       " 'Follow us.',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it!',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Forget it.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get a job.',\n",
       " 'Get ready.',\n",
       " 'Get ready.',\n",
       " 'Go get it.',\n",
       " 'Go get it.',\n",
       " 'Go inside.',\n",
       " 'Go to bed.',\n",
       " 'Go to bed.',\n",
       " 'Good luck.',\n",
       " 'Good luck.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab that.',\n",
       " 'Grab this.',\n",
       " 'Grab this.',\n",
       " 'Hands off.',\n",
       " 'He is ill.',\n",
       " 'He is old.',\n",
       " \"He's a DJ.\",\n",
       " \"He's good.\",\n",
       " \"He's lazy.\",\n",
       " \"He's mine.\",\n",
       " \"He's rich.\",\n",
       " \"He's sexy.\",\n",
       " 'Here I am.',\n",
       " \"Here's $5.\",\n",
       " 'Hold fire.',\n",
       " 'Hold fire.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'Hold this.',\n",
       " 'How awful!',\n",
       " 'How weird!',\n",
       " \"How's Tom?\",\n",
       " \"How's Tom?\",\n",
       " 'I am cold.',\n",
       " 'I am good.',\n",
       " 'I am okay.',\n",
       " 'I am sick.',\n",
       " 'I am sure.',\n",
       " 'I am sure.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I beg you.',\n",
       " 'I can run.',\n",
       " 'I can ski.',\n",
       " 'I cringed.',\n",
       " 'I cringed.',\n",
       " 'I cringed.',\n",
       " 'I exhaled.',\n",
       " 'I gave up.',\n",
       " 'I give in.',\n",
       " 'I give up.',\n",
       " 'I got hot.',\n",
       " 'I got hot.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I had fun.',\n",
       " 'I hate it.',\n",
       " 'I have it.',\n",
       " 'I hit Tom.',\n",
       " 'I hope so.',\n",
       " 'I hurried.',\n",
       " 'I hurried.',\n",
       " 'I inhaled.',\n",
       " 'I knew it.',\n",
       " 'I like it.',\n",
       " 'I lost it.',\n",
       " 'I love it!',\n",
       " 'I love it.',\n",
       " 'I mean it!',\n",
       " 'I mean it.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I must go.',\n",
       " 'I need it.',\n",
       " 'I need it.',\n",
       " 'I noticed.',\n",
       " 'I prepaid.',\n",
       " 'I promise.',\n",
       " 'I relaxed.',\n",
       " 'I relaxed.',\n",
       " 'I retired.',\n",
       " 'I said no.',\n",
       " 'I said so.',\n",
       " 'I saw him.',\n",
       " 'I saw him.',\n",
       " 'I saw him.',\n",
       " 'I saw one.',\n",
       " 'I saw one.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I saw you.',\n",
       " 'I see Tom.',\n",
       " 'I shouted.',\n",
       " 'I tripped.',\n",
       " 'I tripped.',\n",
       " 'I want it.',\n",
       " 'I was new.',\n",
       " 'I was new.',\n",
       " 'I will go.',\n",
       " 'I woke up.',\n",
       " 'I woke up.',\n",
       " \"I'd agree.\",\n",
       " \"I'd leave.\",\n",
       " \"I'll call.\",\n",
       " \"I'll cook.\",\n",
       " \"I'll help.\",\n",
       " \"I'll live.\",\n",
       " \"I'll obey.\",\n",
       " \"I'll pack.\",\n",
       " \"I'll pack.\",\n",
       " \"I'll pack.\",\n",
       " \"I'll pass.\",\n",
       " \"I'll quit.\",\n",
       " \"I'll sing.\",\n",
       " \"I'll stay.\",\n",
       " \"I'll stop.\",\n",
       " \"I'll swim.\",\n",
       " \"I'll talk.\",\n",
       " \"I'll talk.\",\n",
       " \"I'll wait.\",\n",
       " \"I'll walk.\",\n",
       " \"I'll work.\",\n",
       " \"I'll work.\",\n",
       " \"I'm a cop.\",\n",
       " \"I'm a man.\",\n",
       " \"I'm alive.\",\n",
       " \"I'm alive.\",\n",
       " \"I'm alive.\",\n",
       " \"I'm alone.\",\n",
       " \"I'm alone.\",\n",
       " \"I'm angry.\",\n",
       " \"I'm angry.\",\n",
       " \"I'm armed.\",\n",
       " \"I'm armed.\",\n",
       " \"I'm awake.\",\n",
       " \"I'm blind.\",\n",
       " \"I'm broke.\",\n",
       " \"I'm clean.\",\n",
       " \"I'm clean.\",\n",
       " \"I'm crazy.\",\n",
       " \"I'm crazy.\",\n",
       " \"I'm cured.\",\n",
       " \"I'm cured.\",\n",
       " \"I'm dizzy.\",\n",
       " \"I'm drunk.\",\n",
       " \"I'm drunk.\",\n",
       " \"I'm drunk.\",\n",
       " \"I'm dying.\",\n",
       " \"I'm early.\",\n",
       " \"I'm first.\",\n",
       " \"I'm fussy.\",\n",
       " \"I'm fussy.\",\n",
       " \"I'm fussy.\",\n",
       " \"I'm going.\",\n",
       " \"I'm going.\",\n",
       " \"I'm going.\",\n",
       " \"I'm going.\",\n",
       " \"I'm loyal.\",\n",
       " \"I'm loyal.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lucky.\",\n",
       " \"I'm lying.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm naked.\",\n",
       " \"I'm quiet.\",\n",
       " \"I'm ready!\",\n",
       " \"I'm ready!\",\n",
       " \"I'm ready.\",\n",
       " \"I'm right.\",\n",
       " \"I'm sober.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm sorry.\",\n",
       " \"I'm stuck.\",\n",
       " \"I'm timid.\",\n",
       " \"I'm tired.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm tough.\",\n",
       " \"I'm yours.\",\n",
       " \"I'm yours.\",\n",
       " \"I've lost.\",\n",
       " 'Is Tom OK?',\n",
       " 'Is Tom OK?',\n",
       " 'Is it bad?',\n",
       " 'Is it far?',\n",
       " 'Is it far?',\n",
       " 'Is it hot?',\n",
       " 'Is it you?',\n",
       " 'Is it you?',\n",
       " 'Is it you?',\n",
       " 'It failed.',\n",
       " 'It snowed.',\n",
       " 'It stinks.',\n",
       " 'It stinks.',\n",
       " 'It was OK.',\n",
       " 'It was OK.',\n",
       " 'It was OK.',\n",
       " 'It worked.',\n",
       " 'It worked.',\n",
       " \"It's 3:30.\",\n",
       " \"It's 8:30.\",\n",
       " \"It's 8:30.\",\n",
       " \"It's a TV.\",\n",
       " \"It's cold.\",\n",
       " \"It's cold.\",\n",
       " \"It's dark.\",\n",
       " \"It's dead.\",\n",
       " \"It's dead.\",\n",
       " \"It's dead.\",\n",
       " \"It's done.\",\n",
       " \"It's easy.\",\n",
       " \"It's food.\",\n",
       " \"It's free.\",\n",
       " \"It's here.\",\n",
       " \"It's here.\",\n",
       " \"It's hers.\",\n",
       " \"It's hers.\",\n",
       " \"It's late.\",\n",
       " \"It's lost.\",\n",
       " \"It's mine.\",\n",
       " \"It's mine.\",\n",
       " \"It's mine.\",\n",
       " \"It's mine.\",\n",
       " \"It's open.\",\n",
       " \"It's ours.\",\n",
       " \"It's ours.\",\n",
       " \"It's ours.\",\n",
       " \"It's sand.\",\n",
       " \"It's time.\",\n",
       " \"It's time.\",\n",
       " \"It's true!\",\n",
       " \"It's work.\",\n",
       " 'Keep calm.',\n",
       " 'Keep that.',\n",
       " 'Keep that.',\n",
       " 'Keep this.',\n",
       " 'Keep this.',\n",
       " 'Let it be.',\n",
       " 'Let it be.',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go!',\n",
       " 'Let me go.',\n",
       " 'Let me go.',\n",
       " 'Let me go.',\n",
       " 'Let me go.',\n",
       " 'Let me in.',\n",
       " 'Let me in.',\n",
       " \"Let's ask.\",\n",
       " \"Let's eat.\",\n",
       " \"Let's see.\",\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Lie still.',\n",
       " 'Look away.',\n",
       " 'Look away.',\n",
       " 'Look back!',\n",
       " 'Look back!',\n",
       " 'Look here.',\n",
       " 'Look here.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Loosen up.',\n",
       " 'Move over.',\n",
       " 'Move over.',\n",
       " 'Move over.',\n",
       " 'Nice shot!',\n",
       " 'Of course!',\n",
       " 'Of course!',\n",
       " 'Of course!',\n",
       " 'Of course.',\n",
       " 'Of course.',\n",
       " 'Oh please!',\n",
       " 'Oh please!',\n",
       " 'Pardon me?',\n",
       " 'Pardon me?',\n",
       " 'Pardon me?',\n",
       " 'Read this.',\n",
       " 'Say hello.',\n",
       " 'See above.',\n",
       " 'See below.',\n",
       " 'See below.',\n",
       " 'Seize him!',\n",
       " 'Seize him!',\n",
       " 'Seriously?',\n",
       " 'Seriously?',\n",
       " 'Seriously?',\n",
       " 'She cried.',\n",
       " 'She cried.',\n",
       " 'She tried.',\n",
       " 'She walks.',\n",
       " \"She's hot.\",\n",
       " \"She's hot.\",\n",
       " 'Sign here.',\n",
       " 'Sign here.',\n",
       " 'Sign this.',\n",
       " 'Sign this.',\n",
       " 'Slow down.',\n",
       " 'Slow down.',\n",
       " 'Stay away.',\n",
       " 'Stay away.',\n",
       " 'Stay back.',\n",
       " 'Stay back.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay calm.',\n",
       " 'Stay down!',\n",
       " 'Stay down!',\n",
       " 'Stay down.',\n",
       " 'Stay down.',\n",
       " 'Stay here.',\n",
       " 'Stay here.',\n",
       " 'Stay here.',\n",
       " 'Stay thin.',\n",
       " 'Step back.',\n",
       " 'Step back.',\n",
       " 'Stop that!',\n",
       " 'Stop that.',\n",
       " 'Stop that.',\n",
       " 'Stop that.',\n",
       " 'Stop them.',\n",
       " 'Take care!',\n",
       " 'Take care!',\n",
       " 'Take care.',\n",
       " 'Take care.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take mine.',\n",
       " 'Take this.',\n",
       " 'Take this.',\n",
       " 'Thank you.',\n",
       " 'Thank you.',\n",
       " \"That's OK.\",\n",
       " \"That's OK.\",\n",
       " \"That's OK.\",\n",
       " \"That's OK.\",\n",
       " \"That's it.\",\n",
       " 'Then what?',\n",
       " 'They fell.',\n",
       " 'They fell.',\n",
       " 'They left.',\n",
       " 'They left.',\n",
       " 'They lied.',\n",
       " 'They lied.',\n",
       " 'They lost.',\n",
       " 'They lost.',\n",
       " 'They swam.',\n",
       " 'They swam.',\n",
       " 'They swam.',\n",
       " 'They swam.',\n",
       " \"Time's up.\",\n",
       " 'Tom cooks.',\n",
       " 'Tom cried.',\n",
       " 'Tom is OK.',\n",
       " 'Tom is in.',\n",
       " 'Tom knits.',\n",
       " 'Tom knows.',\n",
       " 'Tom rocks.',\n",
       " 'Tom spoke.',\n",
       " 'Tom waved.',\n",
       " 'Tom works.',\n",
       " \"Tom's fat.\",\n",
       " \"Tom's mad.\",\n",
       " \"Tom's mad.\",\n",
       " \"Tom's sad.\",\n",
       " 'Trust Tom.',\n",
       " 'Trust Tom.',\n",
       " 'Try again.',\n",
       " 'Try again.',\n",
       " 'Try again.',\n",
       " 'Try it on.',\n",
       " 'Turn left.',\n",
       " 'Wait here.',\n",
       " 'Wait here.',\n",
       " 'Wait here.',\n",
       " 'Wait here.',\n",
       " 'Watch out!',\n",
       " 'Watch out!',\n",
       " 'Watch out!',\n",
       " 'We agreed.',\n",
       " 'We did it!',\n",
       " 'We did it.',\n",
       " 'We did it.',\n",
       " 'We forgot.',\n",
       " 'We saw it.',\n",
       " 'We saw it.',\n",
       " 'We smiled.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We talked.',\n",
       " 'We waited.',\n",
       " 'We waited.',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as file:\n",
    "    for item in input_texts:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_len = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_len = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of samples: 10000\n",
      "num of unique input tokens: 71\n",
      "num of unique output tokens: 92\n",
      "max seq len for inputs: 16\n",
      "max seq len for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print('num of samples:', len(input_texts))\n",
    "print('num of unique input tokens:', num_encoder_tokens)\n",
    "print('num of unique output tokens:', num_decoder_tokens)\n",
    "print('max seq len for inputs:', max_encoder_seq_len)\n",
    "print('max seq len for outputs:', max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char,i) for i, char in enumerate(input_characters)]\n",
    ")\n",
    "target_token_index = dict(\n",
    "    [(char,i) for i, char in enumerate(target_characters)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts),max_encoder_seq_len, num_encoder_tokens),\n",
    "    dtype = 'float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_len, num_decoder_tokens),    \n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_len, num_decoder_tokens),\n",
    "    dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 16, 71)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encoding\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1\n",
    "    encoder_input_data[i, t+1:, input_token_index[' ']] = 1\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]]=1\n",
    "        if t>0:\n",
    "            decoder_target_data[i, t-1, target_token_index[char]]=1\n",
    "    decoder_input_data[i,t+1:, target_token_index[' ']] = 1\n",
    "    decoder_target_data[i, t:, target_token_index[' ']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 71)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs,_,_ = decoder_lstm(decoder_inputs, initial_state = encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-17 13:58:44.202117: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.6926 - loss: 1.5994 - val_accuracy: 0.6939 - val_loss: 1.1289\n",
      "Epoch 2/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7333 - loss: 1.0026 - val_accuracy: 0.7144 - val_loss: 1.0249\n",
      "Epoch 3/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7534 - loss: 0.8906 - val_accuracy: 0.7156 - val_loss: 0.9957\n",
      "Epoch 4/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7773 - loss: 0.7907 - val_accuracy: 0.7638 - val_loss: 0.8191\n",
      "Epoch 5/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7956 - loss: 0.7113 - val_accuracy: 0.7755 - val_loss: 0.7634\n",
      "Epoch 6/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8079 - loss: 0.6629 - val_accuracy: 0.7908 - val_loss: 0.7199\n",
      "Epoch 7/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8144 - loss: 0.6326 - val_accuracy: 0.7901 - val_loss: 0.7151\n",
      "Epoch 8/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8210 - loss: 0.6113 - val_accuracy: 0.8013 - val_loss: 0.6755\n",
      "Epoch 9/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8290 - loss: 0.5831 - val_accuracy: 0.8108 - val_loss: 0.6466\n",
      "Epoch 10/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8346 - loss: 0.5638 - val_accuracy: 0.8115 - val_loss: 0.6370\n",
      "Epoch 11/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8381 - loss: 0.5502 - val_accuracy: 0.8160 - val_loss: 0.6199\n",
      "Epoch 12/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8445 - loss: 0.5305 - val_accuracy: 0.8222 - val_loss: 0.6052\n",
      "Epoch 13/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8495 - loss: 0.5152 - val_accuracy: 0.8285 - val_loss: 0.5890\n",
      "Epoch 14/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8530 - loss: 0.5019 - val_accuracy: 0.8300 - val_loss: 0.5820\n",
      "Epoch 15/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8563 - loss: 0.4901 - val_accuracy: 0.8324 - val_loss: 0.5684\n",
      "Epoch 16/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8584 - loss: 0.4819 - val_accuracy: 0.8360 - val_loss: 0.5581\n",
      "Epoch 17/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8624 - loss: 0.4656 - val_accuracy: 0.8358 - val_loss: 0.5551\n",
      "Epoch 18/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8635 - loss: 0.4613 - val_accuracy: 0.8404 - val_loss: 0.5379\n",
      "Epoch 19/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8656 - loss: 0.4545 - val_accuracy: 0.8417 - val_loss: 0.5306\n",
      "Epoch 20/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8693 - loss: 0.4412 - val_accuracy: 0.8448 - val_loss: 0.5235\n",
      "Epoch 21/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8723 - loss: 0.4311 - val_accuracy: 0.8476 - val_loss: 0.5173\n",
      "Epoch 22/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8730 - loss: 0.4270 - val_accuracy: 0.8466 - val_loss: 0.5199\n",
      "Epoch 23/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8750 - loss: 0.4195 - val_accuracy: 0.8502 - val_loss: 0.5084\n",
      "Epoch 24/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8770 - loss: 0.4112 - val_accuracy: 0.8511 - val_loss: 0.5025\n",
      "Epoch 25/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8791 - loss: 0.4061 - val_accuracy: 0.8515 - val_loss: 0.5005\n",
      "Epoch 26/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8802 - loss: 0.4028 - val_accuracy: 0.8528 - val_loss: 0.4961\n",
      "Epoch 27/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8822 - loss: 0.3965 - val_accuracy: 0.8555 - val_loss: 0.4904\n",
      "Epoch 28/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8853 - loss: 0.3842 - val_accuracy: 0.8548 - val_loss: 0.4903\n",
      "Epoch 29/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8869 - loss: 0.3805 - val_accuracy: 0.8564 - val_loss: 0.4886\n",
      "Epoch 30/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8878 - loss: 0.3752 - val_accuracy: 0.8589 - val_loss: 0.4779\n",
      "Epoch 31/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8905 - loss: 0.3695 - val_accuracy: 0.8590 - val_loss: 0.4800\n",
      "Epoch 32/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8901 - loss: 0.3666 - val_accuracy: 0.8576 - val_loss: 0.4832\n",
      "Epoch 33/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8926 - loss: 0.3590 - val_accuracy: 0.8593 - val_loss: 0.4769\n",
      "Epoch 34/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8938 - loss: 0.3565 - val_accuracy: 0.8614 - val_loss: 0.4693\n",
      "Epoch 35/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8960 - loss: 0.3474 - val_accuracy: 0.8620 - val_loss: 0.4691\n",
      "Epoch 36/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8985 - loss: 0.3399 - val_accuracy: 0.8622 - val_loss: 0.4674\n",
      "Epoch 37/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8989 - loss: 0.3383 - val_accuracy: 0.8633 - val_loss: 0.4669\n",
      "Epoch 38/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8999 - loss: 0.3326 - val_accuracy: 0.8635 - val_loss: 0.4669\n",
      "Epoch 39/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9004 - loss: 0.3349 - val_accuracy: 0.8637 - val_loss: 0.4669\n",
      "Epoch 40/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9022 - loss: 0.3266 - val_accuracy: 0.8653 - val_loss: 0.4637\n",
      "Epoch 41/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9038 - loss: 0.3213 - val_accuracy: 0.8646 - val_loss: 0.4650\n",
      "Epoch 42/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9052 - loss: 0.3165 - val_accuracy: 0.8650 - val_loss: 0.4627\n",
      "Epoch 43/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9064 - loss: 0.3124 - val_accuracy: 0.8659 - val_loss: 0.4634\n",
      "Epoch 44/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9073 - loss: 0.3104 - val_accuracy: 0.8652 - val_loss: 0.4656\n",
      "Epoch 45/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9091 - loss: 0.3042 - val_accuracy: 0.8668 - val_loss: 0.4614\n",
      "Epoch 46/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9099 - loss: 0.3000 - val_accuracy: 0.8674 - val_loss: 0.4609\n",
      "Epoch 47/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9107 - loss: 0.2972 - val_accuracy: 0.8672 - val_loss: 0.4594\n",
      "Epoch 48/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9121 - loss: 0.2929 - val_accuracy: 0.8670 - val_loss: 0.4597\n",
      "Epoch 49/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9126 - loss: 0.2911 - val_accuracy: 0.8687 - val_loss: 0.4604\n",
      "Epoch 50/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9139 - loss: 0.2856 - val_accuracy: 0.8698 - val_loss: 0.4578\n",
      "Epoch 51/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9150 - loss: 0.2819 - val_accuracy: 0.8689 - val_loss: 0.4592\n",
      "Epoch 52/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9156 - loss: 0.2782 - val_accuracy: 0.8686 - val_loss: 0.4618\n",
      "Epoch 53/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9178 - loss: 0.2738 - val_accuracy: 0.8689 - val_loss: 0.4623\n",
      "Epoch 54/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9167 - loss: 0.2749 - val_accuracy: 0.8693 - val_loss: 0.4594\n",
      "Epoch 55/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9187 - loss: 0.2704 - val_accuracy: 0.8693 - val_loss: 0.4632\n",
      "Epoch 56/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9191 - loss: 0.2685 - val_accuracy: 0.8700 - val_loss: 0.4622\n",
      "Epoch 57/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9207 - loss: 0.2640 - val_accuracy: 0.8712 - val_loss: 0.4578\n",
      "Epoch 58/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9221 - loss: 0.2602 - val_accuracy: 0.8680 - val_loss: 0.4722\n",
      "Epoch 59/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9226 - loss: 0.2584 - val_accuracy: 0.8698 - val_loss: 0.4642\n",
      "Epoch 60/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9235 - loss: 0.2542 - val_accuracy: 0.8709 - val_loss: 0.4643\n",
      "Epoch 61/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9253 - loss: 0.2480 - val_accuracy: 0.8688 - val_loss: 0.4760\n",
      "Epoch 62/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9253 - loss: 0.2483 - val_accuracy: 0.8706 - val_loss: 0.4655\n",
      "Epoch 63/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9264 - loss: 0.2449 - val_accuracy: 0.8640 - val_loss: 0.4912\n",
      "Epoch 64/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9267 - loss: 0.2438 - val_accuracy: 0.8715 - val_loss: 0.4678\n",
      "Epoch 65/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9276 - loss: 0.2414 - val_accuracy: 0.8701 - val_loss: 0.4708\n",
      "Epoch 66/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9282 - loss: 0.2390 - val_accuracy: 0.8720 - val_loss: 0.4697\n",
      "Epoch 67/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9298 - loss: 0.2330 - val_accuracy: 0.8703 - val_loss: 0.4726\n",
      "Epoch 68/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9301 - loss: 0.2308 - val_accuracy: 0.8715 - val_loss: 0.4709\n",
      "Epoch 69/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9315 - loss: 0.2271 - val_accuracy: 0.8708 - val_loss: 0.4754\n",
      "Epoch 70/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9320 - loss: 0.2248 - val_accuracy: 0.8711 - val_loss: 0.4736\n",
      "Epoch 71/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9325 - loss: 0.2237 - val_accuracy: 0.8714 - val_loss: 0.4772\n",
      "Epoch 72/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9337 - loss: 0.2202 - val_accuracy: 0.8709 - val_loss: 0.4788\n",
      "Epoch 73/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9342 - loss: 0.2187 - val_accuracy: 0.8705 - val_loss: 0.4823\n",
      "Epoch 74/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9348 - loss: 0.2148 - val_accuracy: 0.8711 - val_loss: 0.4829\n",
      "Epoch 75/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9359 - loss: 0.2117 - val_accuracy: 0.8704 - val_loss: 0.4849\n",
      "Epoch 76/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9358 - loss: 0.2128 - val_accuracy: 0.8707 - val_loss: 0.4843\n",
      "Epoch 77/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9372 - loss: 0.2085 - val_accuracy: 0.8710 - val_loss: 0.4844\n",
      "Epoch 78/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9266 - loss: 0.2643 - val_accuracy: 0.8702 - val_loss: 0.4903\n",
      "Epoch 79/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9377 - loss: 0.2048 - val_accuracy: 0.8705 - val_loss: 0.4906\n",
      "Epoch 80/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9393 - loss: 0.2013 - val_accuracy: 0.8710 - val_loss: 0.4945\n",
      "Epoch 81/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9397 - loss: 0.2000 - val_accuracy: 0.8705 - val_loss: 0.4973\n",
      "Epoch 82/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9402 - loss: 0.1975 - val_accuracy: 0.8716 - val_loss: 0.4961\n",
      "Epoch 83/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9410 - loss: 0.1952 - val_accuracy: 0.8710 - val_loss: 0.4980\n",
      "Epoch 84/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9420 - loss: 0.1929 - val_accuracy: 0.8704 - val_loss: 0.5008\n",
      "Epoch 85/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9420 - loss: 0.1913 - val_accuracy: 0.8698 - val_loss: 0.5026\n",
      "Epoch 86/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9416 - loss: 0.1921 - val_accuracy: 0.8705 - val_loss: 0.5033\n",
      "Epoch 87/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9430 - loss: 0.1874 - val_accuracy: 0.8712 - val_loss: 0.5049\n",
      "Epoch 88/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9432 - loss: 0.1860 - val_accuracy: 0.8715 - val_loss: 0.5043\n",
      "Epoch 89/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9447 - loss: 0.1817 - val_accuracy: 0.8718 - val_loss: 0.5099\n",
      "Epoch 90/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9443 - loss: 0.1826 - val_accuracy: 0.8717 - val_loss: 0.5094\n",
      "Epoch 91/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9455 - loss: 0.1799 - val_accuracy: 0.8709 - val_loss: 0.5107\n",
      "Epoch 92/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9460 - loss: 0.1767 - val_accuracy: 0.8711 - val_loss: 0.5119\n",
      "Epoch 93/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9467 - loss: 0.1750 - val_accuracy: 0.8704 - val_loss: 0.5188\n",
      "Epoch 94/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9466 - loss: 0.1752 - val_accuracy: 0.8707 - val_loss: 0.5176\n",
      "Epoch 95/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9474 - loss: 0.1724 - val_accuracy: 0.8715 - val_loss: 0.5199\n",
      "Epoch 96/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9481 - loss: 0.1706 - val_accuracy: 0.8707 - val_loss: 0.5219\n",
      "Epoch 97/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9486 - loss: 0.1688 - val_accuracy: 0.8711 - val_loss: 0.5271\n",
      "Epoch 98/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9489 - loss: 0.1664 - val_accuracy: 0.8696 - val_loss: 0.5300\n",
      "Epoch 99/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9494 - loss: 0.1647 - val_accuracy: 0.8688 - val_loss: 0.5347\n",
      "Epoch 100/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9501 - loss: 0.1635 - val_accuracy: 0.8704 - val_loss: 0.5281\n",
      "Epoch 101/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9503 - loss: 0.1635 - val_accuracy: 0.8707 - val_loss: 0.5323\n",
      "Epoch 102/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9511 - loss: 0.1598 - val_accuracy: 0.8702 - val_loss: 0.5356\n",
      "Epoch 103/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9517 - loss: 0.1581 - val_accuracy: 0.8702 - val_loss: 0.5397\n",
      "Epoch 104/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9521 - loss: 0.1560 - val_accuracy: 0.8698 - val_loss: 0.5410\n",
      "Epoch 105/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9522 - loss: 0.1556 - val_accuracy: 0.8706 - val_loss: 0.5381\n",
      "Epoch 106/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9524 - loss: 0.1542 - val_accuracy: 0.8706 - val_loss: 0.5445\n",
      "Epoch 107/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9538 - loss: 0.1510 - val_accuracy: 0.8696 - val_loss: 0.5484\n",
      "Epoch 108/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9540 - loss: 0.1506 - val_accuracy: 0.8697 - val_loss: 0.5512\n",
      "Epoch 109/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9540 - loss: 0.1504 - val_accuracy: 0.8697 - val_loss: 0.5515\n",
      "Epoch 110/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9549 - loss: 0.1478 - val_accuracy: 0.8690 - val_loss: 0.5608\n",
      "Epoch 111/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9544 - loss: 0.1473 - val_accuracy: 0.8693 - val_loss: 0.5578\n",
      "Epoch 112/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9558 - loss: 0.1449 - val_accuracy: 0.8701 - val_loss: 0.5553\n",
      "Epoch 113/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9560 - loss: 0.1429 - val_accuracy: 0.8695 - val_loss: 0.5593\n",
      "Epoch 114/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9565 - loss: 0.1416 - val_accuracy: 0.8701 - val_loss: 0.5601\n",
      "Epoch 115/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9561 - loss: 0.1423 - val_accuracy: 0.8704 - val_loss: 0.5639\n",
      "Epoch 116/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9570 - loss: 0.1395 - val_accuracy: 0.8700 - val_loss: 0.5646\n",
      "Epoch 117/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9579 - loss: 0.1375 - val_accuracy: 0.8700 - val_loss: 0.5676\n",
      "Epoch 118/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9580 - loss: 0.1374 - val_accuracy: 0.8698 - val_loss: 0.5684\n",
      "Epoch 119/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9578 - loss: 0.1363 - val_accuracy: 0.8701 - val_loss: 0.5683\n",
      "Epoch 120/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9585 - loss: 0.1349 - val_accuracy: 0.8682 - val_loss: 0.5767\n",
      "Epoch 121/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9588 - loss: 0.1333 - val_accuracy: 0.8702 - val_loss: 0.5747\n",
      "Epoch 122/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9598 - loss: 0.1303 - val_accuracy: 0.8696 - val_loss: 0.5773\n",
      "Epoch 123/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9596 - loss: 0.1308 - val_accuracy: 0.8694 - val_loss: 0.5807\n",
      "Epoch 124/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9595 - loss: 0.1308 - val_accuracy: 0.8696 - val_loss: 0.5854\n",
      "Epoch 125/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9607 - loss: 0.1275 - val_accuracy: 0.8691 - val_loss: 0.5842\n",
      "Epoch 126/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9608 - loss: 0.1276 - val_accuracy: 0.8698 - val_loss: 0.5866\n",
      "Epoch 127/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9609 - loss: 0.1253 - val_accuracy: 0.8695 - val_loss: 0.5893\n",
      "Epoch 128/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9615 - loss: 0.1245 - val_accuracy: 0.8692 - val_loss: 0.5898\n",
      "Epoch 129/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9616 - loss: 0.1236 - val_accuracy: 0.8692 - val_loss: 0.5972\n",
      "Epoch 130/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9615 - loss: 0.1242 - val_accuracy: 0.8687 - val_loss: 0.5965\n",
      "Epoch 131/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9618 - loss: 0.1228 - val_accuracy: 0.8690 - val_loss: 0.6001\n",
      "Epoch 132/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9622 - loss: 0.1221 - val_accuracy: 0.8696 - val_loss: 0.6018\n",
      "Epoch 133/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9629 - loss: 0.1188 - val_accuracy: 0.8690 - val_loss: 0.6048\n",
      "Epoch 134/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9635 - loss: 0.1179 - val_accuracy: 0.8696 - val_loss: 0.6042\n",
      "Epoch 135/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9639 - loss: 0.1169 - val_accuracy: 0.8695 - val_loss: 0.6089\n",
      "Epoch 136/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9638 - loss: 0.1163 - val_accuracy: 0.8687 - val_loss: 0.6114\n",
      "Epoch 137/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9642 - loss: 0.1148 - val_accuracy: 0.8689 - val_loss: 0.6129\n",
      "Epoch 138/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9642 - loss: 0.1143 - val_accuracy: 0.8698 - val_loss: 0.6134\n",
      "Epoch 139/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9641 - loss: 0.1139 - val_accuracy: 0.8693 - val_loss: 0.6136\n",
      "Epoch 140/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9646 - loss: 0.1129 - val_accuracy: 0.8691 - val_loss: 0.6185\n",
      "Epoch 141/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9651 - loss: 0.1125 - val_accuracy: 0.8695 - val_loss: 0.6145\n",
      "Epoch 142/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9657 - loss: 0.1101 - val_accuracy: 0.8693 - val_loss: 0.6273\n",
      "Epoch 143/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9660 - loss: 0.1091 - val_accuracy: 0.8694 - val_loss: 0.6238\n",
      "Epoch 144/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9664 - loss: 0.1079 - val_accuracy: 0.8686 - val_loss: 0.6265\n",
      "Epoch 145/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9662 - loss: 0.1078 - val_accuracy: 0.8692 - val_loss: 0.6334\n",
      "Epoch 146/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9665 - loss: 0.1065 - val_accuracy: 0.8691 - val_loss: 0.6316\n",
      "Epoch 147/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9668 - loss: 0.1063 - val_accuracy: 0.8695 - val_loss: 0.6315\n",
      "Epoch 148/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9673 - loss: 0.1048 - val_accuracy: 0.8689 - val_loss: 0.6302\n",
      "Epoch 149/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9671 - loss: 0.1041 - val_accuracy: 0.8685 - val_loss: 0.6381\n",
      "Epoch 150/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9680 - loss: 0.1029 - val_accuracy: 0.8685 - val_loss: 0.6396\n",
      "Epoch 151/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9680 - loss: 0.1023 - val_accuracy: 0.8693 - val_loss: 0.6370\n",
      "Epoch 152/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9683 - loss: 0.1020 - val_accuracy: 0.8690 - val_loss: 0.6407\n",
      "Epoch 153/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9684 - loss: 0.1010 - val_accuracy: 0.8686 - val_loss: 0.6449\n",
      "Epoch 154/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9688 - loss: 0.0993 - val_accuracy: 0.8685 - val_loss: 0.6473\n",
      "Epoch 155/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9684 - loss: 0.1000 - val_accuracy: 0.8688 - val_loss: 0.6533\n",
      "Epoch 156/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9683 - loss: 0.1003 - val_accuracy: 0.8691 - val_loss: 0.6500\n",
      "Epoch 157/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9692 - loss: 0.0978 - val_accuracy: 0.8682 - val_loss: 0.6549\n",
      "Epoch 158/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9694 - loss: 0.0971 - val_accuracy: 0.8690 - val_loss: 0.6571\n",
      "Epoch 159/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9698 - loss: 0.0960 - val_accuracy: 0.8689 - val_loss: 0.6618\n",
      "Epoch 160/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9700 - loss: 0.0949 - val_accuracy: 0.8685 - val_loss: 0.6610\n",
      "Epoch 161/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9701 - loss: 0.0950 - val_accuracy: 0.8691 - val_loss: 0.6642\n",
      "Epoch 162/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9702 - loss: 0.0948 - val_accuracy: 0.8683 - val_loss: 0.6676\n",
      "Epoch 163/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9704 - loss: 0.0934 - val_accuracy: 0.8689 - val_loss: 0.6654\n",
      "Epoch 164/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9710 - loss: 0.0926 - val_accuracy: 0.8689 - val_loss: 0.6708\n",
      "Epoch 165/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9707 - loss: 0.0930 - val_accuracy: 0.8680 - val_loss: 0.6765\n",
      "Epoch 166/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9714 - loss: 0.0908 - val_accuracy: 0.8687 - val_loss: 0.6717\n",
      "Epoch 167/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9712 - loss: 0.0907 - val_accuracy: 0.8692 - val_loss: 0.6739\n",
      "Epoch 168/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9715 - loss: 0.0907 - val_accuracy: 0.8680 - val_loss: 0.6792\n",
      "Epoch 169/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9716 - loss: 0.0893 - val_accuracy: 0.8677 - val_loss: 0.6844\n",
      "Epoch 170/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9720 - loss: 0.0883 - val_accuracy: 0.8688 - val_loss: 0.6781\n",
      "Epoch 171/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9723 - loss: 0.0867 - val_accuracy: 0.8685 - val_loss: 0.6834\n",
      "Epoch 172/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9721 - loss: 0.0866 - val_accuracy: 0.8683 - val_loss: 0.6862\n",
      "Epoch 173/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9724 - loss: 0.0862 - val_accuracy: 0.8667 - val_loss: 0.6932\n",
      "Epoch 174/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9726 - loss: 0.0861 - val_accuracy: 0.8684 - val_loss: 0.6861\n",
      "Epoch 175/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9730 - loss: 0.0853 - val_accuracy: 0.8678 - val_loss: 0.6919\n",
      "Epoch 176/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9731 - loss: 0.0845 - val_accuracy: 0.8677 - val_loss: 0.6975\n",
      "Epoch 177/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9734 - loss: 0.0838 - val_accuracy: 0.8684 - val_loss: 0.6990\n",
      "Epoch 178/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9735 - loss: 0.0830 - val_accuracy: 0.8670 - val_loss: 0.7095\n",
      "Epoch 179/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9735 - loss: 0.0824 - val_accuracy: 0.8685 - val_loss: 0.6990\n",
      "Epoch 180/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9739 - loss: 0.0813 - val_accuracy: 0.8676 - val_loss: 0.7098\n",
      "Epoch 181/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9739 - loss: 0.0809 - val_accuracy: 0.8681 - val_loss: 0.7052\n",
      "Epoch 182/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9741 - loss: 0.0804 - val_accuracy: 0.8683 - val_loss: 0.7079\n",
      "Epoch 183/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9743 - loss: 0.0801 - val_accuracy: 0.8670 - val_loss: 0.7118\n",
      "Epoch 184/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9747 - loss: 0.0781 - val_accuracy: 0.8674 - val_loss: 0.7088\n",
      "Epoch 185/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9747 - loss: 0.0790 - val_accuracy: 0.8689 - val_loss: 0.7122\n",
      "Epoch 186/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9745 - loss: 0.0788 - val_accuracy: 0.8674 - val_loss: 0.7182\n",
      "Epoch 187/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9755 - loss: 0.0769 - val_accuracy: 0.8677 - val_loss: 0.7169\n",
      "Epoch 188/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9756 - loss: 0.0757 - val_accuracy: 0.8685 - val_loss: 0.7150\n",
      "Epoch 189/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9753 - loss: 0.0756 - val_accuracy: 0.8677 - val_loss: 0.7205\n",
      "Epoch 190/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9753 - loss: 0.0767 - val_accuracy: 0.8687 - val_loss: 0.7176\n",
      "Epoch 191/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9757 - loss: 0.0743 - val_accuracy: 0.8694 - val_loss: 0.7205\n",
      "Epoch 192/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9755 - loss: 0.0746 - val_accuracy: 0.8673 - val_loss: 0.7359\n",
      "Epoch 193/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9761 - loss: 0.0736 - val_accuracy: 0.8672 - val_loss: 0.7315\n",
      "Epoch 194/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9764 - loss: 0.0726 - val_accuracy: 0.8685 - val_loss: 0.7298\n",
      "Epoch 195/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9761 - loss: 0.0736 - val_accuracy: 0.8676 - val_loss: 0.7365\n",
      "Epoch 196/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9762 - loss: 0.0724 - val_accuracy: 0.8675 - val_loss: 0.7379\n",
      "Epoch 197/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9767 - loss: 0.0720 - val_accuracy: 0.8677 - val_loss: 0.7333\n",
      "Epoch 198/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9768 - loss: 0.0707 - val_accuracy: 0.8674 - val_loss: 0.7397\n",
      "Epoch 199/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9771 - loss: 0.0702 - val_accuracy: 0.8661 - val_loss: 0.7433\n",
      "Epoch 200/200\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9772 - loss: 0.0698 - val_accuracy: 0.8672 - val_loss: 0.7471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7d87461379a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, \n",
    "         batch_size = batch_size,\n",
    "         epochs=epochs,\n",
    "         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs,encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h,decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state= decoder_states_inputs)\n",
    "decoder_states=[state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_inputs]+decoder_states_inputs, [decoder_outputs]+decoder_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index= dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index= dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence  : Go.\n",
      "Decoded Sentence: Va !\n",
      "\n",
      "-\n",
      "Input sentence  : Hi.\n",
      "Decoded Sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence  : Hi.\n",
      "Decoded Sentence: Salut.\n",
      "\n",
      "-\n",
      "Input sentence  : Run!\n",
      "Decoded Sentence: Sarde donc !\n",
      "\n",
      "-\n",
      "Input sentence  : Run!\n",
      "Decoded Sentence: Sarde donc !\n",
      "\n",
      "-\n",
      "Input sentence  : Who?\n",
      "Decoded Sentence: Qui ?\n",
      "\n",
      "-\n",
      "Input sentence  : Wow!\n",
      "Decoded Sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence  : Fire!\n",
      "Decoded Sentence: Au feu !\n",
      "\n",
      "-\n",
      "Input sentence  : Help!\n",
      "Decoded Sentence: Salue-le cons-lains.\n",
      "\n",
      "-\n",
      "Input sentence  : Jump.\n",
      "Decoded Sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence  : Stop!\n",
      "Decoded Sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence  : Stop!\n",
      "Decoded Sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence  : Stop!\n",
      "Decoded Sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence  : Wait!\n",
      "Decoded Sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence  : Wait!\n",
      "Decoded Sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence  : Go on.\n",
      "Decoded Sentence: Continuez.\n",
      "\n",
      "-\n",
      "Input sentence  : Go on.\n",
      "Decoded Sentence: Continuez.\n",
      "\n",
      "-\n",
      "Input sentence  : Go on.\n",
      "Decoded Sentence: Continuez.\n",
      "\n",
      "-\n",
      "Input sentence  : Hello!\n",
      "Decoded Sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence  : Hello!\n",
      "Decoded Sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence  : I see.\n",
      "Decoded Sentence: Je les vois.\n",
      "\n",
      "-\n",
      "Input sentence  : I try.\n",
      "Decoded Sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence  : I won!\n",
      "Decoded Sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence  : I won!\n",
      "Decoded Sentence: Je l'ai emporté !\n",
      "\n",
      "-\n",
      "Input sentence  : I won.\n",
      "Decoded Sentence: Je l'ai cassé.\n",
      "\n",
      "-\n",
      "Input sentence  : Oh no!\n",
      "Decoded Sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence  : Attack!\n",
      "Decoded Sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence  : Attack!\n",
      "Decoded Sentence: Attaquez !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Cheers!\n",
      "Decoded Sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence  : Get up.\n",
      "Decoded Sentence: Lâche-toi !\n",
      "\n",
      "-\n",
      "Input sentence  : Go now.\n",
      "Decoded Sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Go now.\n",
      "Decoded Sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Go now.\n",
      "Decoded Sentence: Vas-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence  : Got it!\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence  : Got it!\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence  : Got it?\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence  : Got it?\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence  : Got it?\n",
      "Decoded Sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence  : Hop in.\n",
      "Decoded Sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence  : Hop in.\n",
      "Decoded Sentence: Montez.\n",
      "\n",
      "-\n",
      "Input sentence  : Hug me.\n",
      "Decoded Sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence  : Hug me.\n",
      "Decoded Sentence: Serrez-moi dans vos bras !\n",
      "\n",
      "-\n",
      "Input sentence  : I fell.\n",
      "Decoded Sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input sentence  : I fell.\n",
      "Decoded Sentence: Je suis tombé.\n",
      "\n",
      "-\n",
      "Input sentence  : I know.\n",
      "Decoded Sentence: Je la vais.\n",
      "\n",
      "-\n",
      "Input sentence  : I left.\n",
      "Decoded Sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input sentence  : I left.\n",
      "Decoded Sentence: Je suis parti.\n",
      "\n",
      "-\n",
      "Input sentence  : I lied.\n",
      "Decoded Sentence: Je le suis mien de vous.\n",
      "\n",
      "-\n",
      "Input sentence  : I lost.\n",
      "Decoded Sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence  : I paid.\n",
      "Decoded Sentence: J'ai passé.\n",
      "\n",
      "-\n",
      "Input sentence  : I'm 19.\n",
      "Decoded Sentence: Je suis tout à fait prête.\n",
      "\n",
      "-\n",
      "Input sentence  : I'm OK.\n",
      "Decoded Sentence: Je suis tout à fait prête.\n",
      "\n",
      "-\n",
      "Input sentence  : I'm OK.\n",
      "Decoded Sentence: Je suis tout à fait prête.\n",
      "\n",
      "-\n",
      "Input sentence  : Listen.\n",
      "Decoded Sentence: Grouplez !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : No way!\n",
      "Decoded Sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence  : Really?\n",
      "Decoded Sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence  : Really?\n",
      "Decoded Sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence  : Really?\n",
      "Decoded Sentence: Vrai ?\n",
      "\n",
      "-\n",
      "Input sentence  : Thanks.\n",
      "Decoded Sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence  : We try.\n",
      "Decoded Sentence: Nous sommes tout sobre.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence  : We won.\n",
      "Decoded Sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence  : Ask Tom.\n",
      "Decoded Sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence  : Awesome!\n",
      "Decoded Sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence  : Be calm.\n",
      "Decoded Sentence: Sois calme !\n",
      "\n",
      "-\n",
      "Input sentence  : Be calm.\n",
      "Decoded Sentence: Sois calme !\n",
      "\n",
      "-\n",
      "Input sentence  : Be calm.\n",
      "Decoded Sentence: Sois calme !\n",
      "\n",
      "-\n",
      "Input sentence  : Be cool.\n",
      "Decoded Sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be fair.\n",
      "Decoded Sentence: Soyez juste !\n",
      "\n",
      "-\n",
      "Input sentence  : Be kind.\n",
      "Decoded Sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Be nice.\n",
      "Decoded Sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence  : Beat it.\n",
      "Decoded Sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence  : Call me.\n",
      "Decoded Sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence  : Call me.\n",
      "Decoded Sentence: Appelle-moi !\n",
      "\n",
      "-\n",
      "Input sentence  : Call us.\n",
      "Decoded Sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence  : Call us.\n",
      "Decoded Sentence: Appelez-nous !\n",
      "\n",
      "-\n",
      "Input sentence  : Come in.\n",
      "Decoded Sentence: Entrez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "    \n",
    "    target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    \n",
    "    target_seq[0, 0, target_token_index['\\t']]=1.\n",
    "    \n",
    "    stop_condition= False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h,c = decoder_model.predict([target_seq]+states_value, verbose=0)\n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "       \n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        \n",
    "        decoded_sentence += sampled_char\n",
    "        if (sampled_char=='\\n' or len(decoded_sentence)>max_decoder_seq_len):\n",
    "            stop_condition=True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0,0, sampled_token_index]=1.\n",
    "        \n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence\n",
    "\n",
    "for seq_index in range(100):\n",
    "    input_seq = encoder_input_data[seq_index:seq_index+1]\n",
    "    decode_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence  :', input_texts[seq_index])\n",
    "    print('Decoded Sentence:', decode_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 802062,
     "sourceId": 1375271,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30197,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
